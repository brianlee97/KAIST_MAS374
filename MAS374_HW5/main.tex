\documentclass[11pt]{article}
\usepackage[margin=0.7in]{geometry} 
\usepackage{amsmath,amsthm,amssymb,amsfonts, amsbsy}
\usepackage{enumitem}
\usepackage{dsfont}
\usepackage{soul}
\usepackage{mathtools}
\usepackage[utf8]{inputenc}
\usepackage{multirow}
\usepackage[colorlinks]{hyperref}
\usepackage{cleveref}
\usepackage{bbm}
\usepackage{tikz-cd}
\usepackage{adjustbox}
\usepackage[normalem]{ulem}
\usepackage{authblk}
\usepackage{listings}
\usepackage{xcolor}
\usepackage{graphicx}

\renewcommand{\baselinestretch}{1.25}

\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}

\lstdefinestyle{mystyle}{
    backgroundcolor=\color{backcolour},   
    commentstyle=\color{codegreen},
    keywordstyle=\color{magenta},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\ttfamily\footnotesize,
    breakatwhitespace=false,         
    breaklines=true,                 
    captionpos=b,                    
    keepspaces=true,                 
    numbers=left,                    
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=2
}

\lstset{style=mystyle}
 
\newtheorem{lemma}{Lemma}
\newtheorem{claim}{\sf Claim}
\newtheorem{defi}{Definition}
\newtheorem{thm}{Theorem}
\newtheorem{cor}{Corollary}
\newtheorem{prop}{Proposition}
\newtheorem{rmk}{\it Remark}
\newtheorem{ex}{Example}
\newtheorem{notation}{Notation}
\newtheorem{algorithm}{Algorithm}
\newtheorem{assumption}{Assumption}
\newtheorem{problem}{Problem}

\DeclareMathOperator*{\argmin}{argmin}
\DeclareMathOperator*{\argmax}{argmax}

\numberwithin{equation}{problem}
  
\begin{document}
 
\title{MAS374 Optimization Theory\\ Homework \#5}
\author{20150597 Jeonghwan Lee}
\affil{Department of Mathematical Sciences, KAIST}

\maketitle

\begin{problem} [\emph{Exercise 8.4} in \cite{calafiore2014optimization}]
\label{problem1}
\normalfont{\ \\
\indent (1) Define $f : \mathbb{R}^2 \rightarrow \left[ - \infty, +\infty \right)$ by $f(\alpha, \beta) := \inf \left\{ \alpha d + \frac{\beta^2}{d} : d \in \left( 0, +\infty \right) \right\}$. Let us consider the following three cases:
\begin{itemize}
    \item $\alpha < 0$: It holds for any $t \in \left[ 1, +\infty \right)$ that
    \begin{equation}
        \label{eqn1.1}
        \begin{split}
            f(\alpha, \beta) &\leq \inf \left\{ \alpha d + \frac{\beta^2}{d} : d \in \left[ 1, +\infty \right) \right\} \\
            &\leq \inf \left\{ \alpha d + \beta^2 : d \in \left[ 1, +\infty \right) \right\} \\
            &\leq \alpha t + \beta^2.
        \end{split}
    \end{equation}
    By letting $t \to \ifnty$ in the inequality \eqref{eqn1.1}, we have
    \begin{equation*}
        f(\alpha, \beta) \leq \lim_{t \to \infty} \left( \alpha t + \beta^2 \right) = -\infty,
    \end{equation*}
    since $\alpha < 0$. Thus, $f (\alpha, \beta) = -\infty$ for every $(\alpha, \beta) \in \left( - \infty, 0 \right) \times \mathbb{R}$;
    \item $\alpha = 0$: If $\beta = 0$, then it's clear that $f(0, 0) = 0$ and the optimal value $f(0, 0) = 0$ is attained at every feasible point $d \in \left( 0, +\infty \right)$. If $\beta \neq 0$, then it holds that
    \begin{equation*}
        f(0, \beta) = \inf \left\{ \frac{\beta^2}{d} : d \in \left( 0, +\infty \right) \right\} = 0,
    \end{equation*}
    and the optimal value $f(0, \beta) = 0$ cannot be attained at any feasible point in $\left( 0, +\infty \right)$. Therefore, we have $f(0, \beta) = 0$ for any $\beta \in \mathbb{R}$;
    \item $\alpha > 0$: We may observe that
    \begin{equation}
        \label{eqn1.2}
        \alpha d + \frac{\beta^2}{d} = \left( \sqrt{\alpha d} - \frac{\left| \beta \right|}{\sqrt{d}} \right)^2 + 2 \left| \beta \right| \sqrt{\alpha},\ \forall d \in \left( 0, +\infty \right).
    \end{equation}
    From the identity \eqref{eqn1.2}, one can see that
    \begin{equation*}
        f(\alpha, \beta) = \inf \left\{ \left( \sqrt{\alpha d} - \frac{\left| \beta \right|}{\sqrt{d}} \right)^2 + 2 \left| \beta \right| \sqrt{\alpha} : d \in \left( 0, +\infty \right) \right\}
        = 2 \left| \beta \right| \sqrt{\alpha},
    \end{equation*}
    and this optimal value is attained at $d^* = \frac{\left| \beta \right|}{\sqrt{\alpha}}$, for every $(\alpha, \beta) \in \left( 0, +\infty \right) \times \mathbb{R}$.
\end{itemize}

\noindent To sum up, we arrive at
\begin{equation*}
    f(\alpha, \beta) =
    \begin{cases}
        - \infty & \textnormal{if } (\alpha, \beta) \in \left( -\infty, 0 \right) \times \mathbb{R}; \\
        2 \left| \beta \right| \sqrt{\alpha} & \textnormal{otherwise,}
    \end{cases}
\end{equation*}
as desired. 
\medskip

\indent (2) To begin with, we define the objective function $g_{\mathbf{z}}(\cdot) : \left( 0, +\infty \right)^m \rightarrow \mathbb{R}$ by
\begin{equation*}
    g_{\mathbf{z}} \left( d_1, d_2, \cdots, d_m \right) := \frac{1}{2}
    \sum_{i=1}^{m} \left( d_i + \frac{z_{i}^2}{d_i} \right),\ \forall \mathbf{z} \in \mathbb{R}^m.
\end{equation*}
Then for any $\mathbf{d} = \left( d_1, d_2, \cdots, d_m \right) \in \left( 0, +\infty \right)^m$, we have
\begin{equation*}
    \begin{split}
        g_{\mathbf{z}}(\mathbf{d}) &\geq \frac{1}{2} \sum_{i=1}^{m} \inf \left\{ t_i + \frac{z_{i}^2}{t_i} : t_i \in \left( 0, +\infty \right) \right\} \\
        &= \frac{1}{2} \sum_{i=1}^{m} f \left( 1, z_i \right) \\
        &\stackrel{\textnormal{(a)}}{=} \sum_{i=1}^{m} \left| z_i \right| \\
        &= \left\| \mathbf{z} \right\|_1,
    \end{split}
\end{equation*}
where the step (a) follows from the part (1), thereby
\begin{equation}
    \label{eqn1.3}
    \inf \left\{ g_{\mathbf{z}}(\mathbf{d}) : \mathbf{d} \in \left( 0, +\infty \right)^m \right\} \geq \left\| \mathbf{z} \right\|_1.
\end{equation}
\indent On the other hand, let $\mathbf{d}^{(k)} \in \left( 0, +\infty \right)^m$ be given by
\begin{equation*}
    d_{i}^{(k)} :=
    \begin{cases}
        \left| z_i \right| & \textnormal{if } i \in \mathcal{S}(\mathbf{z}); \\
        \frac{1}{k} & \textnormal{otherwise,}
    \end{cases}
\end{equation*}
where $\mathcal{S}(\mathbf{z}) := \left\{ j \in [m] : z_j \neq 0 \right\}$ denotes the support of the vector $\mathbf{z} \in \mathbb{R}^m$. Then,
\begin{equation*}
    \begin{split}
        g_{\mathbf{z}} \left( \mathbf{d}^{(k)} \right)
        &= \frac{1}{2} \sum_{i \in \mathcal{S}(\mathbf{z})} 2 \left| z_i \right| + \frac{1}{2} \sum_{i \in [m] \setminus \mathcal{S}(\mathbf{z})} \frac{1}{k} \\
        &= \sum_{i \in \mathcal{S}(\mathbf{z})} \left| z_i \right|
        + \frac{m - \left| \mathcal{S}(\mathbf{z}) \right|}{2k} \\
        &= \sum_{i=1}^{m} \left| z_i \right|
        + \frac{m - \left| \mathcal{S}(\mathbf{z}) \right|}{2k} \\
        &= \left\| \mathbf{z} \right\|_1
        + \frac{m - \left| \mathcal{S}(\mathbf{z}) \right|}{2k}
    \end{split}
\end{equation*}
for every $k \in \mathbb{N}$. So it follows that
\begin{equation}
    \label{eqn1.4}
    \inf \left\{ g_{\mathbf{z}}(\mathbf{d}) : \mathbf{d} \in \left( 0, +\infty \right)^m \right\} \leq g_{\mathbf{z}} \left( \mathbf{d}^{(k)} \right) = \left\| \mathbf{z} \right\|_1
        + \frac{m - \left| \mathcal{S}(\mathbf{z}) \right|}{2k}
\end{equation}
for every $k \in \mathbb{N}$. By letting $k \to \infty$ in the bound \eqref{eqn1.4}, we obtain
\begin{equation}
    \label{eqn1.5}
    \inf \left\{ g_{\mathbf{z}}(\mathbf{d}) : \mathbf{d} \in \left( 0, +\infty \right)^m \right\} \leq \left\| \mathbf{z} \right\|_1.
\end{equation}
Taking two pieces \eqref{eqn1.3} and \eqref{eqn1.5} collectively, we may conclude that
\begin{equation}
    \label{eqn1.6}
    \inf \left\{ g_{\mathbf{z}}(\mathbf{d}) : \mathbf{d} \in \left( 0, +\infty \right)^m \right\} = \left\| \mathbf{z} \right\|_1
\end{equation}
as desired. Note that the optimization problem
\begin{equation}
    \label{eqn1.7}
    p_{1}^* := \min_{\mathbf{d} \in \left( 0, +\infty \right)^m} g_{\mathbf{z}}(\mathbf{d}) = \frac{1}{2}
    \sum_{i=1}^{m} \left( d_i + \frac{z_{i}^2}{d_i} \right)
\end{equation}
has the optimal value $p_{1}^* = \left\| \mathbf{z} \right\|_1$ from \eqref{eqn1.6}. For any $\mathbf{d} \in \left( 0, +\infty \right)^m$, one can see that
\begin{equation}
    \label{eqn1.8}
    g_{\mathbf{z}}(\mathbf{d}) - \left\| \mathbf{z} \right\|_1
    = \frac{1}{2} \sum_{i=1}^{m} \frac{1}{d_i} \left( d_i - \left| z_i \right| \right)^2
    = \frac{1}{2} \left[ \sum_{i \in \mathcal{S}(\mathbf{z})} \frac{1}{d_i} \left( d_i - \left| z_i \right| \right)^2 + \sum_{i \in [m] \setminus \mathcal{S}(\mathbf{z})} d_i \right].
\end{equation}
Owing to the identity \eqref{eqn1.8}, one can make the following conclusion: If $[m] \setminus \mathcal{S}(\mathbf{z}) = \varnothing$, then the optimization problem \eqref{eqn1.7} has an optimal solution $\mathbf{d}^* = \left( \left| z_1 \right|, \left| z_2 \right|, \cdots, \left| z_m \right| \right) \in \left( 0, +\infty \right)^m$. Otherwise, the identity \eqref{eqn1.8} yields
\begin{equation*}
    g_{\mathbf{z}}(\mathbf{d}) - \left\| \mathbf{z} \right\|_1 \geq
    \frac{1}{2} \sum_{i \in [m] \setminus \mathcal{S}(\mathbf{z})} d_i > 0
\end{equation*}
for every $\mathbf{d} \in \left( 0, +\infty \right)^m$, and this implies that the optimal value $p_{1}^* = \left\| \mathbf{z} \right\|_1$ of the optimization problem \eqref{eqn1.7} cannot be attained at any feasible point $\mathbf{d} \in \left( 0, +\infty \right)^m$ whenever $[m] \setminus \mathcal{S}(\mathbf{z}) \neq \varnothing$. In brief,
\begin{enumerate} [label=(\roman*)]
    \item $[m] \setminus \mathcal{S}(\mathbf{z}) = \varnothing$: the optimization problem \eqref{eqn1.7} attains an optimal solution $\mathbf{d}^* = \left( \left| z_1 \right|, \left| z_2 \right|, \cdots, \left| z_m \right| \right) \in \left( 0, +\infty \right)^m$;
    \item Otherwise: the optimization problem \eqref{eqn1.7} does not attain any optimal solutions.
\end{enumerate}

\indent (3) Let us define the objective function $h_{\mathbf{z}} (\cdot) : \left( 0, +\infty \right)^m \rightarrow \mathbb{R}$ by
\begin{equation*}
    h_{\mathbf{z}} \left( d_1, d_2, \cdots, d_m \right) :=
    \sum_{i=1}^{m} \frac{z_{i}^2}{d_i}.
\end{equation*}
Also, let $\mathcal{X} := \left\{ \mathbf{d} = \left( d_1, d_2, \cdots, d_m \right) \in \left( 0, +\infty \right)^m : \sum_{i=1}^{m} d_i = 1 \right\}$. For any $\mathbf{d} \in \mathcal{X}$, we obtain from the Cauchy-Schwarz inequality that
\begin{equation*}
    \begin{split}
        h_{\mathbf{z}}(\mathbf{d}) = \left( \sum_{i=1}^{m} d_i \right) \left( \sum_{i=1}^{m} \frac{z_{i}^2}{d_i} \right)
        \geq \left( \sum_{i=1}^{m} \left| z_i \right| \right)^2
        = \left\| \mathbf{z} \right\|_{1}^2,
    \end{split}
\end{equation*}
thereby it holds that
\begin{equation}
    \label{eqn1.9}
    \inf \left\{ h_{\mathbf{z}}(\mathbf{d}) : \mathbf{d} \in \mathcal{X} \right\} \geq \left\| \mathbf{z} \right\|_{1}^2.
\end{equation}
\indent On the other hand, we first consider the case where $\mathbf{z} = \mathbf{0} \in \mathbb{R}^m$. Then it's clear that $\left\| \mathbf{z} \right\|_1 = 0 = h_{\mathbf{z}} (\mathbf{d})$ for all $\mathbf{d} \in \mathcal{X}$, and we are done! So we may assume that $\mathbf{z} \in \mathbb{R}^m \setminus \left\{ \mathbf{0} \right\}$. For $k \geq 2$, we define $\mathbf{d}^{(k)} \in \left( 0, +\infty \right)^m$ by
\begin{equation*}
    d_{i}^{(k)} :=
    \begin{cases}
        \left( 1 - \frac{1}{k} \right) \cdot \frac{\left| z_i \right|}{\left\| \mathbf{z} \right\|_1} & \textnormal{if } i \in \mathcal{S}(\mathbf{z}); \\
        \frac{1}{m - \left| \mathcal{S}(\mathbf{z}) \right|} \cdot \frac{1}{k} & \textnormal{otherwise,}
    \end{cases}
\end{equation*}
where $\mathcal{S}(\mathbf{z}) := \left\{ j \in [m]: z_j \neq 0 \right\}$ denotes the support of the vector $\mathbf{z} \in \mathbb{R}^m \setminus \left\{ \mathbf{0} \right\}$. Then,
\begin{equation*}
    \begin{split}
        \sum_{i=1}^{m} d_{i}^{(k)} &= \sum_{i \in \mathcal{S}(\mathbf{z})} d_{i}^{(k)} + \sum_{i \in [m] \setminus \mathcal{S}(\mathbf{z})} d_{i}^{(k)} \\
        &= \left( 1 - \frac{1}{k} \right) \cdot \frac{1}{\left\| \mathbf{z} \right\|_1} \sum_{i \in \mathcal{S}(\mathbf{z})} \left| z_i \right| + \frac{1}{k} \\
        &= \left( 1 - \frac{1}{k} \right) + \frac{1}{k} \\
        &= 1,
    \end{split}
\end{equation*}
which ensures that $\mathbf{d}^{(k)} \in \mathcal{X}$ for every $k \geq 2$. Also, one can see that
\begin{equation*}
    \begin{split}
        h_{\mathbf{z}} \left( \mathbf{d}^{(k)} \right) &=
        \sum_{i \in \mathcal{S}(\mathbf{z})} \frac{z_{i}^2}{d_{i}^{(k)}} \\
        &= \frac{k}{k-1} \cdot \left\| \mathbf{z} \right\|_1 \left( \sum_{i \in \mathcal{S}(\mathbf{z})} \left| z_i \right| \right) \\
        &= \frac{k}{k-1} \cdot \left\| \mathbf{z} \right\|_{1}^2.
    \end{split}
\end{equation*}
Therefore, we obtain
\begin{equation}
    \label{eqn1.10}
    \begin{split}
        \inf \left\{ h_{\mathbf{z}}(\mathbf{d}) : \mathbf{d} \in \mathcal{X} \right\} \leq h_{\mathbf{z}} \left( \mathbf{d}^{(k)} \right) = \frac{k}{k-1} \cdot \left\| \mathbf{z} \right\|_{1}^2
        \stackrel{k \to \infty}{\longrightarrow} \left\| \mathbf{z} \right\|_{1}^2.
    \end{split}
\end{equation}
Taking two pieces \eqref{eqn1.9} and \eqref{eqn1.10} collectively, one has
\begin{equation}
    \label{eqn1.11}
    \inf \left\{ h_{\mathbf{z}}(\mathbf{d}) : \mathbf{d} \in \mathcal{X} \right\} = \left\| \mathbf{z} \right\|_{1}^2,
\end{equation}
as desired. Here, we note that the optimization problem
\begin{equation}
    \label{eqn1.12}
    p_{2}^* := \min_{\mathbf{d} \in \mathcal{X}}
    h_{\mathbf{z}}(\mathbf{d}) = \sum_{i=1}^{m} \frac{z_{i}^2}{d_i}
\end{equation}
has the optimal value $p_{2}^* = \left\| \mathbf{z} \right\|_{1}^2$ from \eqref{eqn1.11}. For any $\mathbf{d} \in \mathcal{X}$, it holds that
\begin{equation}
    \label{eqn1.13}
    \begin{split}
        h_{\mathbf{z}}(\mathbf{d}) - \left\| \mathbf{z} \right\|_{1}^2
        &= \left( \sum_{i=1}^{m} \frac{z_{i}^2}{d_i} \right) \left( \sum_{i=1}^{m} d_i \right) - \left( \sum_{i=1}^{m} \left| z_i \right| \right)^2 \\
        &= \sum_{1 \leq i < j \leq m} \left( \frac{\left| z_i \right|}{\sqrt{d_i}} \cdot \sqrt{d_j} - \frac{\left| z_j \right|}{\sqrt{d_j}} \cdot \sqrt{d_i} \right)^2 \\
        &= \sum_{1 \leq i < j \leq m} \frac{\left( \left| z_i \right| d_j - \left| z_j \right| d_i \right)^2}{d_i d_j}.
    \end{split}
\end{equation}
Due to the identity \eqref{eqn1.13}, one can make the following conclusions: If $[m] \setminus \mathcal{S} (\mathbf{z}) = \varnothing$, then the optimization problem \eqref{eqn1.12} has an optimal solution $\mathbf{d}^* = \left( \frac{\left| z_1 \right|}{\left\| \mathbf{z} \right\|_1}, \frac{\left| z_2 \right|}{\left\| \mathbf{z} \right\|_1}, \cdots, \frac{\left| z_m \right|}{\left\| \mathbf{z} \right\|_1} \right) \in \mathcal{X}$. Otherwise, we have $z_i = 0$ for some $i \in [m]$. We claim that the optimization problem \eqref{eqn1.12} does not attain any optimal solutions provided that $[m] \setminus \mathcal{S} (\mathbf{z}) \neq \varnothing$ and $\mathbf{z} \in \mathbb{R}^m \setminus \left\{ \mathbf{0} \right\}$. Assume towards a contradiction that the optimization problem \eqref{eqn1.12} has an optimal solution $\mathbf{d}^* \in \mathcal{X}$. From the identity \eqref{eqn1.13}, we arrive at
\begin{equation*}
    \sum_{1 \leq i < j \leq m} \frac{\left( \left| z_i \right| d_{j}^* - \left| z_j \right| d_{i}^* \right)^2}{d_{i}^* d_{j}^*}
    = 0,
\end{equation*}
which implies that $z_j = 0$ for all $j \in [m]$ and thus we obtain a contradiction! To sum up,
\begin{enumerate}[label=(\roman*)]
    \item $[m] \setminus \mathcal{S} (\mathbf{z}) = \varnothing$: the optimization problem \eqref{eqn1.12} has an optimal solution $\mathbf{d}^* = \left( \frac{\left| z_1 \right|}{\left\| \mathbf{z} \right\|_1}, \frac{\left| z_2 \right|}{\left\| \mathbf{z} \right\|_1}, \cdots, \frac{\left| z_m \right|}{\left\| \mathbf{z} \right\|_1} \right) \in \mathcal{X}$;
    \item $[m] \setminus \mathcal{S} (\mathbf{z}) = [m]$, \emph{i.e.}, $\mathbf{z} = \mathbf{0} \in \mathbb{R}^m$: any feasible point in $\mathcal{X}$ is an optimal solution to the optimization problem \eqref{eqn1.12}; 
    \item Otherwise: the optimal value $p_{2}^* = \left\| \mathbf{z} \right\|_{1}^2$ of the optimization problem \eqref{eqn1.12} cannot be attained at any feasible points,
\end{enumerate}
and this completes our discussion of the part (3).
}
\end{problem}

\begin{problem} [\emph{Exercise 8.7} in \cite{calafiore2014optimization}]
\label{problem2}
\normalfont{\ \\
\indent (1) We claim that the function $\phi_p (\cdot) : \mathbb{R}^{n \times m} \rightarrow \mathbb{R}_{+}$ is a norm on $\mathbb{R}^{n \times m}$.
\medskip

\indent \textbf{Positive definiteness of $\phi_p (\cdot)$:} It's clear that $\phi_p \left( \mathbf{O}_{n \times m} \right) = 0$, where $\mathbf{O}_{n \times m} \in \mathbb{R}^{n \times m}$ denotes the $n \times m$ all-zero matrix. Conversely, we assume that
\begin{equation*}
    \phi_p (\mathbf{X}) = 0 = \max \left\{ \left\| \mathbf{X}^{\top} \mathbf{u} \right\|_p : \mathbf{u} \in \mathbb{S}^{n-1} \right\},
\end{equation*}
where $\mathbb{S}^{n-1} := \left\{ \mathbf{v} \in \mathbb{R}^n : \left\| \mathbf{v} \right\|_2 = 1 \right\}$ denotes the unit $(n-1)$-sphere. We claim that $\mathbf{X} = \mathbf{O}_{n \times m}$. To this end, we assume on a contrary that $\mathbf{X} \neq \mathbf{O}_{n \times m}$. Then $\mathbf{x}_i \in \mathbb{R}^n \setminus \left\{ \mathbf{0} \right\}$ for some $i \in [m]$, thereby
\begin{equation*}
    \begin{split}
        0 \geq \left\| \mathbf{X}^{\top} \cdot \frac{\mathbf{x}_i}{\left\| \mathbf{x}_i \right\|_2} \right\|_p 
        &= \left\| \mathbf{x}_i \right\|_{2}^{-1} \cdot \left\| \mathbf{X}^{\top} \mathbf{x}_i \right\|_p \\
        &= \left\| \mathbf{x}_i \right\|_{2}^{-1} \cdot
        \left\| \begin{bmatrix} \mathbf{x}_{1}^{\top} \mathbf{x}_i \\
        \mathbf{x}_{2}^{\top} \mathbf{x}_i \\ \vdots \\ \mathbf{x}_{m}^{\top} \mathbf{x}_i \end{bmatrix} \right\|_p \\
        &\geq \left\| \mathbf{x}_i \right\|_{2}^{-1} \cdot \left| \mathbf{x}_{i}^{\top} \mathbf{x}_i \right| \\
        &= \left\| \mathbf{x}_i \right\|_2,
    \end{split}
\end{equation*}
and this implies $\mathbf{x}_i = \mathbf{0}$, contradiction! So we have $\mathbf{X} = \mathbf{O}_{n \times m}$ and this establishes the positive definiteness of the map $\phi_p (\cdot) : \mathbb{R}^{n \times m} \rightarrow \mathbb{R}_{+}$.
\medskip

\indent \textbf{Sub-additivity of $\phi_p (\cdot)$:} For any $\mathbf{X}, \mathbf{Y} \in \mathbb{R}^{n \times m}$, it holds that
\begin{equation*}
    \begin{split}
        \phi_p (\mathbf{X} + \mathbf{Y}) &=
        \max \left\{ \left\| \left( \mathbf{X} + \mathbf{Y} \right)^{\top} \mathbf{u} \right\|_p : \mathbf{u} \in \mathbb{S}^{n-1} \right\} \\
        &\stackrel{\textnormal{(a)}}{\leq} \max \left\{ \left\| \mathbf{X}^{\top} \mathbf{u} \right\|_p + \left\| \mathbf{Y}^{\top} \mathbf{u} \right\|_p : \mathbf{u} \in \mathbb{S}^{n-1} \right\} \\
        &\leq \max \left\{ \left\| \mathbf{X}^{\top} \mathbf{u} \right\|_p : \mathbf{u} \in \mathbb{S}^{n-1} \right\} + \max \left\{ \left\| \mathbf{Y}^{\top} \mathbf{u} \right\|_p : \mathbf{u} \in \mathbb{S}^{n-1} \right\} \\
        &= \phi_p (\mathbf{X}) + \phi_p (\mathbf{Y}),
    \end{split}
\end{equation*}
where the step (a) follows from the fact that $\left\| \cdot \right\|_p$ is a norm on $\mathbb{R}^m$, and this establishes the sub-additivity of the map $\phi_p (\cdot) : \mathbb{R}^{n \times m} \rightarrow \mathbb{R}_{+}$.
\medskip

\indent \textbf{Absolute homogeneity of $\phi_p (\cdot)$:} For any $\mathbf{X} \in \mathbb{R}^{n \times m}$ and $\alpha \in \mathbb{R}$, one has
\begin{equation*}
    \begin{split}
        \phi_p (\alpha \mathbf{X}) &= 
        \max \left\{ \left\| \left( \alpha \mathbf{X} \right)^{\top} \mathbf{u} \right\|_p : \mathbf{u} \in \mathbb{S}^{n-1} \right\} \\
        &= \max \left\{ \left| \alpha \right| \left\| \mathbf{X}^{\top} \mathbf{u} \right\|_p : \mathbf{u} \in \mathbb{S}^{n-1} \right\} \\
        &= \left| \alpha \right| \max \left\{ \left\| \mathbf{X}^{\top} \mathbf{u} \right\|_p : \mathbf{u} \in \mathbb{S}^{n-1} \right\} \\
        &= \left| \alpha \right| \phi_p (\mathbf{X}),
    \end{split}
\end{equation*}
and this establishes the absolute homogeneity of the map $\phi_p (\cdot) : \mathbb{R}^{n \times m} \rightarrow \mathbb{R}_{+}$.
\medskip

\indent Taking the above arguments collectively, we deduce that $\phi_p (\cdot) : \mathbb{R}^{n \times m} \rightarrow \mathbb{R}_{+}$ is a norm on $\mathbb{R}^{n \times m}$. 
\medskip

\indent (2) We consider the following optimization problem:
\begin{equation}
    \label{eqn2.1}
    \begin{split}
        \phi_2 (\mathbf{X}) = \max_{\mathbf{u} \in \mathbb{R}^n} \ & \left\| \mathbf{X}^{\top} \mathbf{u} \right\|_2 \\
        \textnormal{subject to }& \left\| \mathbf{u} \right\|_2 = 1.
    \end{split}
\end{equation}
One can observe that the objective function of the optimization problem \eqref{eqn2.1} can be expressed as
\begin{equation}
    \label{eqn2.2}
    \begin{split}
        \left\| \mathbf{X}^{\top} \mathbf{u} \right\|_{2}^2
        &= \sum_{i=1}^{m} \left( \mathbf{x}_{i}^{\top} \mathbf{u} \right)^2 \\
        &= \sum_{i=1}^{m} \left( \mathbf{x}_{i}^{\top} \mathbf{u} \right)^{\top} \left( \mathbf{x}_{i}^{\top} \mathbf{u} \right) \\
        &= \sum_{i=1}^{m} \mathbf{u}^{\top} \left( \mathbf{x}_{i} \mathbf{x}_{i}^{\top} \right) \mathbf{u} \\
        &= \mathbf{u}^{\top} \left( \sum_{i=1}^{m} \mathbf{x}_{i} \mathbf{x}_{i}^{\top} \right) \mathbf{u} \\
        &= \mathbf{u}^{\top} \left( \mathbf{X} \mathbf{X}^{\top} \right) \mathbf{u}.
    \end{split}
\end{equation}
At this point, let $\mathbf{X} = \mathbf{U}_r \mathbf{\Sigma} \mathbf{V}_{r}^{\top} = \sum_{j=1}^{r} \sigma_j (\mathbf{X}) \mathbf{u}_j \mathbf{v}_{j}^{\top}$ be the compact-form \textsf{SVD} of $\mathbf{X}$, where $r := \textsf{rank}(\mathbf{X}) \leq \min \left\{ m, n \right\}$, $\mathbf{U}_r := \begin{bmatrix} \mathbf{u}_1 & \mathbf{u}_2 & \cdots & \mathbf{u}_r \end{bmatrix} \in \mathbb{R}^{n \times r}$, $\mathbf{V}_r := \begin{bmatrix} \mathbf{v}_1 & \mathbf{v}_2 & \cdots & \mathbf{v}_r \end{bmatrix} \in \mathbb{R}^{m \times r}$, and
\begin{equation*}
    \Sigma := \textsf{diag} \left( \sigma_1 (\mathbf{X}), \sigma_2 (\mathbf{X}), \cdots, \sigma_r (\mathbf{X}) \right) \in \mathbb{R}^{r \times r}; \quad \sigma_1 (\mathbf{X}) \geq \sigma_2 (\mathbf{X}) \geq \cdots \geq \sigma_r (\mathbf{X}) > 0.
\end{equation*}
Then we have
\begin{equation*}
    \mathbf{X} \mathbf{X}^{\top} =
    \sum_{j=1}^{r} \sigma_j (\mathbf{X})^2 \cdot \mathbf{u}_j \mathbf{u}_{j}^{\top},
\end{equation*}
thereby the equation \eqref{eqn2.2} becomes
\begin{equation*}
    \begin{split}
        \left\| \mathbf{X}^{\top} \mathbf{u} \right\|_{2}^2 &= \mathbf{u}^{\top} \left( \sum_{j=1}^{r} \sigma_j (\mathbf{X})^2 \cdot \mathbf{u}_j \mathbf{u}_{j}^{\top} \right) \mathbf{u} \\
        &= \sum_{j=1}^{r} \sigma_j (\mathbf{X})^2 \cdot \left( \mathbf{u}_{j}^{\top} \mathbf{u} \right)^2 \\
        &\leq \sigma_{1}(\mathbf{X})^2 \left( \sum_{j=1}^{r} \left( \mathbf{u}_{j}^{\top} \mathbf{u} \right)^2 \right) \\
        &= \sigma_{1}(\mathbf{X})^2 \left( \sum_{j=1}^{n} \left( \mathbf{u}_{j}^{\top} \mathbf{u} \right)^2 \right) \\
        &= \sigma_{1}(\mathbf{X})^2 \left\{ \mathbf{u}^{\top} \left( \sum_{j=1}^{n} \mathbf{u}_j \mathbf{u}_{j}^{\top} \right) \mathbf{u} \right\} \\
        &= \sigma_{1}(\mathbf{X})^2 \left( \mathbf{u}^{\top} \mathnf{u} \right) \\
        &= \sigma_{1}(\mathbf{X})^2,
    \end{split}
\end{equation*}
where $\left\{ \mathbf{u}_{r+1}, \mathbf{u}_{r+2}, \cdots, \mathbf{u}_{n} \right\}$ is an orthonormal basis of $\left( \textsf{span} \left( \left\{ \mathbf{u}_1, \mathbf{u}_2, \cdots, \mathbf{u}_r \right\} \right) \right)^{\perp}$, thereby $\left\{ \mathbf{u}_1, \mathbf{u}_2, \cdots, \mathbf{u}_n \right\}$ forms an orthonormal basis for $\mathbb{R}^n$. Therefore, we have
\begin{equation}
    \label{eqn2.3}
    \sup \left\{ \left\| \mathbf{X}^{\top} \mathbf{u} \right\|_2 : \mathbf{u} \in \mathbb{S}^{n-1} \right\} \leq \sigma_1 (\mathbf{X}) = \sigma_{\textsf{max}}(\mathbf{X}) = \left\| \mathbf{X} \right\|_{2 \to 2}.
\end{equation}
It's straightforward that $\left\| \mathbf{X}^{\top} \mathbf{u}_1 \right\|_2 = \sigma_1 (\mathbf{X}) = \sigma_{\textsf{max}}(\mathbf{X}) = \left\| \mathbf{X} \right\|_{2 \to 2}$. Thus, the equality holds in the bound \eqref{eqn2.3}, the optimal value of the optimization problem \eqref{eqn2.1} is $\phi_2 (\mathbf{X}) = \sigma_1 (\mathbf{X}) = \sigma_{\textsf{max}}(\mathbf{X}) = \left\| \mathbf{X} \right\|_{2 \to 2}$, and the optimization problem \eqref{eqn2.1} has an optimal solution $\mathbf{u}^* = \mathbf{u}_1$, \emph{i.e.},
\begin{equation*}
    \mathbf{u}_1 \in \argmax \left\{ \left\| \mathbf{X}^{\top} \mathbf{u} \right\|_2 : \mathbf{u} \in \mathbb{S}^{n-1} \right\},
\end{equation*}
where $\mathbf{u}_1 \in \mathbb{S}^{n-1}$ refers to the first left singular vector of the matrix $\mathbf{X} \in \mathbb{R}^{n \times m}$.
\medskip

\indent (3) We consider the following optimization problem:
\begin{equation}
    \label{eqn2.4}
    \begin{split}
        \phi_{\infty} (\mathbf{X}) = \max_{\mathbf{u} \in \mathbb{R}^n} \ & \left\| \mathbf{X}^{\top} \mathbf{u} \right\|_{\infty} \\
        \textnormal{subject to }& \left\| \mathbf{u} \right\|_2 = 1.
    \end{split}
\end{equation}
For any $\mathbf{u} \in \mathbb{S}^{n-1}$, it holds that
\begin{equation*}
    \begin{split}
        \left\| \mathbf{X}^{\top} \mathbf{u} \right\|_{\infty}
        &= \max \left\{ \left| \mathbf{x}_{i}^{\top} \mathbf{u} \right| : i \in [m] \right\} \\
        &\stackrel{\textnormal{(b)}}{\leq} \max \left\{ \left\| \mathbf{x}_{i} \right\|_{2} \cdot \left\| \mathbf{u} \right\|_2 : i \in [m] \right\} \\
        &= \max \left\{ \left\| \mathbf{x}_{i} \right\|_{2} : i \in [m] \right\},
    \end{split}
\end{equation*}
where the step (b) holds due to the Cauchy-Schwarz inequality. Therefore, we arrive at
\begin{equation}
    \label{eqn2.5}
    \sup \left\{ \left\| \mathbf{X}^{\top} \mathbf{u} \right\|_{\infty} : \mathbf{u} \in \mathbb{S}^{n-1} \right\} \leq \max \left\{ \left\| \mathbf{x}_{i} \right\|_{2} : i \in [m] \right\}.
\end{equation}
\indent On the other hand, let $j^* \in \argmax \left\{ \left\| \mathbf{x}_i \right\|_2 : i \in [m] \right\}$. If $\mathbf{X} = \mathbf{O}_{n \times m}$, it's evident that $\phi_{\infty} \left( \mathbf{O}_{n \times m} \right) = 0$ and any feasible point of the optimization problem \eqref{eqn2.4} is its optimal solution. So we now may assume that $\mathbf{X} \in \mathbb{R}^{n \times m} \setminus \left\{ \mathbf{O}_{n \times m} \right\}$. Then $\mathbf{x}_{j^*} \in \mathbb{R}^n \setminus \left\{ \mathbf{0} \right\}$ and
\begin{equation*}
    \left\| \mathbf{X}^{\top} \cdot \frac{\mathbf{x}_{j^*}}{\left\| \mathbf{x}_{j^*} \right\|_2} \right\|_{\infty} \geq 
    \left| \mathbf{x}_{j^*}^{\top} \cdot \frac{\mathbf{x}_{j^*}}{\left\| \mathbf{x}_{j^*} \right\|_2} \right| 
    = \left\| \mathbf{x}_{j^*} \right\|_2
    = \max \left\{ \left\| \mathbf{x}_i \right\|_2 : i \in [m] \right\},
\end{equation*}
which yields
\begin{equation}
    \label{eqn2.6}
    \sup \left\{ \left\| \mathbf{X}^{\top} \mathbf{u} \right\|_{\infty} : \mathbf{u} \in \mathbb{S}^{n-1} \right\} \geq \left\| \mathbf{X}^{\top} \cdot \frac{\mathbf{x}_{j^*}}{\left\| \mathbf{x}_{j^*} \right\|_2} \right\|_{\infty} 
    \geq \max \left\{ \left\| \mathbf{x}_i \right\|_2 : i \in [m] \right\}.
\end{equation}
Taking two pieces \eqref{eqn2.5} and \eqref{eqn2.6} collectively, we find that
\begin{equation}
    \label{eqn2.7}
    \max \left\{ \left\| \mathbf{X}^{\top} \mathbf{u} \right\|_{\infty} : \mathbf{u} \in \mathbb{S}^{n-1} \right\} = \max \left\{ \left\| \mathbf{x}_{i} \right\|_{2} : i \in [m] \right\},
\end{equation}
and the maximum in the left-hand side of the equation \eqref{eqn2.7} is attained at an optimal solution $\mathbf{u}^* = \frac{\mathbf{x}_{j^*}}{\left\| \mathbf{x}_{j^*} \right\|_2}$. Hence, the optimal value of the optimization problem \eqref{eqn2.4} is $\phi_{\infty}(\mathbf{X}) = \max \left\{ \left\| \mathbf{x}_{i} \right\|_{2} : i \in [m] \right\}$ (notice that this result also holds for the case where $\mathbf{X} = \mathbf{O}_{n \times m}$), and the optimization problem \eqref{eqn2.4} has an optimal solution $\mathbf{u}^* = \frac{\mathbf{x}_{j^*}}{\left\| \mathbf{x}_{j^*} \right\|_2}$, \emph{i.e.},
\begin{equation*}
    \frac{\mathbf{x}_{j^*}}{\left\| \mathbf{x}_{j^*} \right\|_2} \in \argmax  \left\{ \left\| \mathbf{X}^{\top} \mathbf{u} \right\|_{\infty} : \mathbf{u} \in \mathbb{S}^{n-1} \right\},
\end{equation*}
where $j^* \in \argmax \left\{ \left\| \mathbf{x}_i \right\|_2 : i \in [m] \right\}$.
\medskip

\indent (4) Let $\theta_p (\cdot) : \mathbb{R}^{n \times m} \rightarrow \mathbb{R}_{+}$ be defined by
\begin{equation}
    \label{eqn2.8}
    \theta_p (\mathbf{X}) := \max \left\{ \left\| \mathbf{X} \mathbf{v} \right\|_2 : \mathbf{v} \in \mathbb{R}^m \textnormal{ such that } \left\| \mathbf{v} \right\|_q \leq 1 \right\}.
\end{equation}
Note that the map $\theta_p (\cdot) : \mathbb{R}^{n \times m} \rightarrow \mathbb{R}_{+}$ is well-defined since the function $\mathbf{v} \in \mathbb{R}^m \mapsto \left\| \mathbf{X} \mathbf{v} \right\|_2$ is a continuous function and $\left\{ \mathbf{v} \in \mathbb{R}^m: \left\| \mathbf{v} \right\|_q \leq 1 \right\}$ is a compact subset of $\mathbb{R}^m$. Then we have
\begin{equation}
    \label{eqn2.9}
    \begin{split}
        \phi_p (\mathbf{X}) &= \max \left\{ \left\| \mathbf{X}^{\top} \mathbf{u} \right\|_p : \mathbf{u} \in \mathbb{S}^{n-1} \right\} \\
        &\stackrel{\textnormal{(c)}}{\leq} \max \left\{ \max \left\{ \left( \mathbf{X}^{\top} \mathbf{u} \right)^{\top} \mathbf{v} : \mathbf{v} \in \mathbb{R}^m \textnormal{ such that } \left\| \mathbf{v} \right\|_q \leq 1 \right\} : \mathbf{u} \in \mathbb{S}^{n-1} \right\} \\
        &= \max \left\{ \max \left\{ \mathbf{u}^{\top} \mathbf{X} \mathbf{v} : \mathbf{v} \in \mathbb{R}^m \textnormal{ such that } \left\| \mathbf{v} \right\|_q \leq 1 \right\} : \mathbf{u} \in \mathbb{S}^{n-1} \right\} \\
        &\stackrel{\textnormal{(d)}}{\leq} \max \left\{ \max \left\{ \left\| \mathbf{u} \right\|_2 \cdot \left\| \mathbf{Xv} \right\|_2 : \mathbf{v} \in \mathbb{R}^m \textnormal{ such that } \left\| \mathbf{v} \right\|_q \leq 1 \right\} : \mathbf{u} \in \mathbb{S}^{n-1} \right\} \\
        &= \max \left\{ \cdot \left\| \mathbf{Xv} \right\|_2 : \mathbf{v} \in \mathbb{R}^m \textnormal{ such that } \left\| \mathbf{v} \right\|_q \leq 1 \right\} \\
        &= \theta_p (\mathbf{X}),
    \end{split}
\end{equation}
where the step (c) follows from the fact
\begin{equation}
    \label{eqn2.10}
    \left\| \mathbf{x} \right\|_p = \max \left\{ \mathbf{x}^{\top} \mathbf{y} : \mathbf{y} \in \mathbb{R}^n \textnormal{ such that } \left\| \mathbf{y} \right\|_q \leq 1 \right\}
\end{equation}
for any $\mathbf{x} \in \mathbb{R}^n$ and $p, q \in \left[ 1, +\infty \right]$ satisfying the relation $\frac{1}{p} + \frac{1}{q} = 1$, and the step (d) holds by the Cauchy-Schwarz inequality. 
\medskip

\indent On the other hand, let $\mathbf{v}^* \in \argmax \left\{ \left\| \mathbf{X} \mathbf{v} \right\|_2 : \mathbf{v} \in \mathbb{R}^m \textnormal{ such that } \left\| \mathbf{v} \right\|_q \leq 1 \right\}$. Then,
\begin{equation}
    \label{eqn2.11}
    \begin{split}
        \theta_p (\mathbf{X}) &= \left\| \mathbf{X} \mathbf{v}^* \right\|_2 \\
        &\stackrel{\textnormal{(e)}}{=} \max \left\{ \mathbf{u}^{\top} \mathbf{X} \mathbf{v}^* : \mathbf{u} \in \mathbb{S}^{n-1} \right\} \\
        &= \max \left\{ \left( \mathbf{X}^{\top} \mathbf{u} \right)^{\top} \mathbf{v}^* : \mathbf{u} \in \mathbb{S}^{n-1} \right\} \\
        &\stackrel{\textnormal{(f)}}{\leq} \max \left\{ \left\| \mathbf{X}^{\top} \mathbf{u} \right\|_p \cdot \left\| \mathbf{v}^* \right\|_q : \mathbf{u} \in \mathbb{S}^{n-1} \right\} \\
        &\stackrel{\textnormal{(g)}}{\leq} \max \left\{ \left\| \mathbf{X}^{\top} \mathbf{u} \right\|_p : \mathbf{u} \in \mathbb{S}^{n-1} \right\} \\
        &= \phi_p (\mathbf{X}),
    \end{split}
\end{equation}
where the step (e) makes use of the fact \eqref{eqn2.10}, the step (f) holds due to the H\"{o}lder's inequality, and the step (g) follows from the fact $\left\| \mathbf{v}^* \right\|_q \leq 1$. By combining two results \eqref{eqn2.9} and \eqref{eqn2.11}, we arrive at
\begin{equation*}
    \phi_p (\mathbf{X}) = \theta_p (\mathbf{X}) = \max \left\{ \left\| \mathbf{X} \mathbf{v} \right\|_2 : \mathbf{v} \in \mathbb{R}^m \textnormal{ such that } \left\| \mathbf{v} \right\|_q \leq 1 \right\},
\end{equation*}
as desired. This completes the solution to Problem \ref{problem2}.
}
\end{problem}

\newpage

\bibliographystyle{plain}
\bibliography{main.bib}

\end{document}
