\documentclass[11pt]{article}
\usepackage[margin=0.7in]{geometry} 
\usepackage{amsmath,amsthm,amssymb,amsfonts, amsbsy}
\usepackage{enumitem}
\usepackage{dsfont}
\usepackage{soul}
\usepackage{mathtools}
\usepackage[utf8]{inputenc}
\usepackage{multirow}
\usepackage[colorlinks]{hyperref}
\usepackage{cleveref}
\usepackage{bbm}
\usepackage{tikz-cd}
\usepackage{adjustbox}
\usepackage[normalem]{ulem}
\usepackage{authblk}
\usepackage{listings}
\usepackage{xcolor}
\usepackage{graphicx}

\renewcommand{\baselinestretch}{1.25}

\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}

\lstdefinestyle{mystyle}{
    backgroundcolor=\color{backcolour},   
    commentstyle=\color{codegreen},
    keywordstyle=\color{magenta},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\ttfamily\footnotesize,
    breakatwhitespace=false,         
    breaklines=true,                 
    captionpos=b,                    
    keepspaces=true,                 
    numbers=left,                    
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=2
}

\lstset{style=mystyle}
 
\newtheorem{lemma}{Lemma}
\newtheorem{claim}{\sf Claim}
\newtheorem{defi}{Definition}
\newtheorem{thm}{Theorem}
\newtheorem{cor}{Corollary}
\newtheorem{prop}{Proposition}
\newtheorem{rmk}{\it Remark}
\newtheorem{ex}{Example}
\newtheorem{notation}{Notation}
\newtheorem{algorithm}{Algorithm}
\newtheorem{assumption}{Assumption}
\newtheorem{problem}{Problem}

\DeclareMathOperator*{\argmin}{argmin}
\DeclareMathOperator*{\argmax}{argmax}

\numberwithin{equation}{problem}
  
\begin{document}
 
\title{MAS374 Optimization Theory\\ Homework \#8}
\author{20150597 Jeonghwan Lee\footnote{E-mail: \href{mailto:sa8seung@kaist.ac.kr}{\texttt{sa8seung@kaist.ac.kr}}}}
\affil{Department of Mathematical Sciences, KAIST}

\maketitle

\begin{problem} [\emph{Exercise 11.1} in \cite{calafiore2014optimization}]
\label{problem1}
\normalfont{\ \\
\indent (1) Let $\mathcal{L} \left( \mathbf{p}; \mathbf{q} \right) := \left\{ \left( 1 - \theta \right) \mathbf{p} + \theta \mathbf{q} : \theta \in \left[ 0, 1 \right] \right\} \subseteq \mathbb{R}^n$ for $\mathbf{p} \neq \mathbf{q}$ in $\mathbb{R}^n$. Then,
\begin{equation*}
    \begin{split}
        \mathcal{D} \left( \mathbf{p}; \mathbf{q} \right) &:= \left( \textnormal{the minimum distance from the origin $\mathbf{0} \in \mathbb{R}^n$ to the line segment } \mathcal{L} \left( \mathbf{p}; \mathbf{q} \right) \right) \\
        &= \min \left\{ \left\| \lambda \mathbf{p} + \left( 1 - \lambda \right) \mathbf{q} \right\|_2 : \lambda \in \left[ 0, 1 \right] \right\}.
    \end{split}
\end{equation*}
Thus, it holds for every $R \geq \mathbb{R}_{+}$ that
\begin{equation}
    \label{eqn1.1}
    \begin{split}
        \mathcal{D} \left( \mathbf{p}; \mathbf{q} \right) \geq R \quad &\Leftrightarrow \quad 
        \left\| \lambda \mathbf{p} + \left( 1 - \lambda \right) \mathbf{q} \right\|_2 \geq R,\ \forall \lambda \in \left[ 0, 1 \right] \\
        &\Leftrightarrow \quad \left\| \lambda \mathbf{p} + \left( 1 - \lambda \right) \mathbf{q} \right\|_{2}^2 \geq R^2,\ \forall \lambda \in \mathbb{R} \textnormal{ such that } \lambda \left( 1 - \lambda \right) \leq 0,
    \end{split}
\end{equation}
which precisely yields the desired result. 
\medskip

\indent (2) From \eqref{eqn1.1}, it immediately follows that
\begin{equation}
    \label{eqn1.2}
    \mathcal{D} \left( \mathbf{p}; \mathbf{q} \right) \geq R \quad \Leftrightarrow \quad 
    \left\{ \lambda \in \mathbb{R} : \lambda \left( \lambda - 1 \right) \leq 0 \right\} \subseteq 
    \left\{ \lambda \in \mathbb{R}: R^2 -
    \left\| \lambda \mathbf{p} + \left( 1 - \lambda \right) \mathbf{q} \right\|_{2}^2 \leq 0 \right\}.
\end{equation}
At this point, we define two quadratic functions $f_0 (\cdot) : \mathbb{R} \rightarrow \mathbb{R}$ and $f_1 (\cdot) : \mathbb{R} \rightarrow \mathbb{R}$ by
\begin{equation*}
    \begin{split}
        f_0 (\lambda) &:= R^2 -
        \left\| \lambda \mathbf{p} + \left( 1 - \lambda \right) \mathbf{q} \right\|_{2}^2
        = F_0 \lambda^2 + 2 g_0 \lambda + h_0; \\
        f_1 (\lambda) &:= \lambda \left( \lambda - 1 \right)
        = F_1 \lambda^2 + 2 g_1 \lambda + h_1,
    \end{split}
\end{equation*}
where $F_0 := - \left\| \mathbf{p} - \mathbf{q} \right\|_{2}^2$, $g_0 := \mathbf{q}^{\top} \left( \mathbf{q} - \mathbf{p} \right)$, $h_0 := R^2 - \mathbf{q}^{\top} \mathbf{q}$, and $F_1 := 1$, $g_1 := - \frac{1}{2}$, $h_1 := 0$. Since the inequality $f_1 (\lambda) \leq 0$ is strictly feasible, \emph{i.e.}, $f_1 \left( \tilde{\lambda} \right) < 0$ for some $\tilde{\lambda} \in \mathbb{R}$, the \emph{lossless $\textnormal{\textsf{S}}$-procedure} is valid for two quadratic functions $f_0 (\cdot) : \mathbb{R} \rightarrow \mathbb{R}$ and $f_1 (\cdot) : \mathbb{R} \rightarrow \mathbb{R}$: for every $R \in \mathbb{R}_{+}$,
\begin{equation}
    \label{eqn1.3}
    \begin{split}
        \mathcal{D} \left( \mathbf{p}; \mathbf{q} \right) \geq R \quad &\Leftrightarrow \quad 
        \left\{ \lambda \in \mathbb{R} : \lambda \left( \lambda - 1 \right) \leq 0 \right\} \subseteq 
        \left\{ \lambda \in \mathbb{R}: R^2 -
        \left\| \lambda \mathbf{p} + \left( 1 - \lambda \right) \mathbf{q} \right\|_{2}^2 \leq 0 \right\} \\
        &\Leftrightarrow \quad
        \begin{bmatrix}
            - \left\| \mathbf{p} - \mathbf{q} \right\|_{2}^2 & \mathbf{q}^{\top} \left( \mathbf{q} - \mathbf{p} \right) \\
            \mathbf{q}^{\top} \left( \mathbf{q} - \mathbf{p} \right) & R^2 - \mathbf{q}^{\top} \mathbf{q}
        \end{bmatrix}
        \preceq \tau
        \begin{bmatrix}
            1 & - \frac{1}{2} \\ - \frac{1}{2} & 0
        \end{bmatrix}
        \textnormal{ for some $\tau \in \mathbb{R}_{+}$} \\
        &\Leftrightarrow \quad
        \begin{bmatrix}
            \tau + \left\| \mathbf{p} - \mathbf{q} \right\|_{2}^2 & \mathbf{q}^{\top} \left( \mathbf{p} - \mathbf{q} \right) - \frac{\tau}{2} \\
            \mathbf{q}^{\top} \left( \mathbf{p} - \mathbf{q} \right) - \frac{\tau}{2} & \mathbf{q}^{\top} \mathbf{q} - R^2
        \end{bmatrix}
        \in \mathcal{S}_{+}^2 \textnormal{ for some $\tau \in \mathbb{R}_{+}$,}
    \end{split}
\end{equation}
where $\mathcal{S}_{+}^d$ denotes the convex cone consists of $d \times d$ real, symmetric, positive semi-definite matrices, and this completes the proof of the part (2).
\medskip

\indent (3) From \eqref{eqn1.3}, it's clear that
\begin{equation}
    \label{eqn1.4}
    \left[ 0, \mathcal{D} \left( \mathbf{p}; \mathbf{q} \right) \right]
    = \underbrace{\left\{ R \in \mathbb{R}_{+} : 
    \begin{bmatrix}
        \tau + \left\| \mathbf{p} - \mathbf{q} \right\|_{2}^2 & \mathbf{q}^{\top} \left( \mathbf{p} - \mathbf{q} \right) - \frac{\tau}{2} \\
        \mathbf{q}^{\top} \left( \mathbf{p} - \mathbf{q} \right) - \frac{\tau}{2} & \mathbf{q}^{\top} \mathbf{q} - R^2
    \end{bmatrix}
    \in \mathcal{S}_{+}^2 \textnormal{ for some $\tau \in \mathbb{R}_{+}$} \right\}}_{=: \ \Omega \ \subseteq \ \mathbb{R}_{+}}.
\end{equation}
If $R \in \Omega$, then 
\begin{equation*}
    \mathbf{q}^{\top} \mathbf{q} - R^2
    = \begin{bmatrix} 0 \\ 1 \end{bmatrix}^{\top}
    \begin{bmatrix}
        \tau + \left\| \mathbf{p} - \mathbf{q} \right\|_{2}^2 & \mathbf{q}^{\top} \left( \mathbf{p} - \mathbf{q} \right) - \frac{\tau}{2} \\
        \mathbf{q}^{\top} \left( \mathbf{p} - \mathbf{q} \right) - \frac{\tau}{2} & \mathbf{q}^{\top} \mathbf{q} - R^2
    \end{bmatrix}
    \begin{bmatrix} 0 \\ 1 \end{bmatrix}
    \geq 0,
\end{equation*}
thereby $R \leq \left( \mathbf{q}^{\top} \mathbf{q} \right)^{\frac{1}{2}}$. Thus, we have $\mathcal{D} \left( \mathbf{p}; \mathbf{q} \right) \leq \left( \mathbf{q}^{\top} \mathbf{q} \right)^{\frac{1}{2}}$.
\medskip

\indent \textbf{Case \#1.} $\mathbf{p}^{\top} \mathbf{q} \geq \mathbf{q}^{\top} \mathbf{q}$: It's clear that $\left( \mathbf{q}^{\top} \mathbf{q} \right)^{\frac{1}{2}} \in \Omega$, since
\begin{equation*}
    \begin{bmatrix}
        \tau + \left\| \mathbf{p} - \mathbf{q} \right\|_{2}^2 & \mathbf{q}^{\top} \left( \mathbf{p} - \mathbf{q} \right) - \frac{\tau}{2} \\
        \mathbf{q}^{\top} \left( \mathbf{p} - \mathbf{q} \right) - \frac{\tau}{2} & 0
    \end{bmatrix}
    \in \mathcal{S}_{+}^2 \quad \textnormal{for }
    \tau = 2 \mathbf{q}^{\top} \left( \mathbf{p} - \mathbf{q} \right) \in \mathbb{R}_{+}.
\end{equation*}
This implies $\mathcal{D} \left( \mathbf{p}; \mathbf{q} \right) \geq \left( \mathbf{q}^{\top} \mathbf{q} \right)^{\frac{1}{2}}$, thereby we have
\begin{equation}
    \label{eqn1.5}
    \mathcal{D} \left( \mathbf{p}; \mathbf{q} \right) = \left( \mathbf{q}^{\top} \mathbf{q} \right)^{\frac{1}{2}},
\end{equation}
provided that $\mathbf{p}^{\top} \mathbf{q} \geq \mathbf{q}^{\top} \mathbf{q}$.
\medskip

\indent \textbf{Case \#2.} $\mathbf{p}^{\top} \mathbf{q} \geq \mathbf{p}^{\top} \mathbf{p}$: Since $\underbrace{\left( \mathbf{p}^{\top} \mathbf{p} - \mathbf{p}^{\top} \mathbf{q} \right)}_{\leq \ 0} + \left( \mathbf{q}^{\top} \mathbf{q} - \mathbf{p}^{\top} \mathbf{q} \right) = \left\| \mathbf{p} - \mathbf{q} \right\|_{2}^2 > 0$, we find that
\begin{equation}
    \label{eqn1.6}
    \mathbf{p}^{\top} \mathbf{p} \leq \mathbf{p}^{\top} \mathbf{q} < \mathbf{q}^{\top} \mathbf{q}.
\end{equation}
If $\left( \mathbf{q}^{\top} \mathbf{q} \right)^{\frac{1}{2}} \in \Omega$, then
\begin{equation}
    \label{eqn1.7}
    \textsf{det} \left(
    \begin{bmatrix}
        \tau + \left\| \mathbf{p} - \mathbf{q} \right\|_{2}^2 & \mathbf{q}^{\top} \left( \mathbf{p} - \mathbf{q} \right) - \frac{\tau}{2} \\
        \mathbf{q}^{\top} \left( \mathbf{p} - \mathbf{q} \right) - \frac{\tau}{2} & 0
    \end{bmatrix}
    \right)
    = - \left\{ \mathbf{q}^{\top} \left( \mathbf{p} - \mathbf{q} \right) - \frac{\tau}{2} \right\}^2 \geq 0
\end{equation}
for some $\tau \in \mathbb{R}_{+}$. However, the inequality \eqref{eqn1.6} cannot hold since the inequality \eqref{eqn1.5} implies $2 \mathbf{q}^{\top} \left( \mathbf{p} - \mathbf{q} \right) < 0$, contradiction! Thus, $\mathcal{D} \left( \mathbf{p}; \mathbf{q} \right) < \left( \mathbf{q}^{\top} \mathbf{q} \right)^{\frac{1}{2}}$ and $\mathbf{q}^{\top} \mathbf{q} - R^2 > 0$ for every $R \in \Omega = \left[ 0, \mathcal{D} \left( \mathbf{p}; \mathbf{q} \right) \right]$. Due to the Schur's complement rule, one can obtain by doing some straightforward algebra that
\begin{equation}
    \label{eqn1.8}
    \begin{split}
        \Omega &=
        \left\{ R \in \mathbb{R}_{+} : \tau + \left\| \mathbf{p} - \mathbf{q} \right\|_{2}^2 - \frac{\left\{ \mathbf{q}^{\top} \left( \mathbf{p} - \mathbf{q} \right) - \frac{\tau}{2} \right\}^2}{\mathbf{q}^{\top} \mathbf{q} - R^2} \geq 0 \textnormal{ for some } \tau \in \mathbb{R}_{+} \right\} \\
        &= \left\{ R \in \mathbb{R}_{+} : \left\{ \tau - 2 \left( \mathbf{q}^{\top} \mathbf{p} - R^2 \right) \right\} - 4 \left( R^2 - \mathbf{p}^{\top} \mathbf{p} \right) \left( R^2 - \mathbf{q}^{\top} \mathbf{q} \right) \leq 0 \textnormal{ for some } \tau \in \mathbb{R}_{+} \right\} \\
        &= \left\{ R \in \mathbb{R}_{+} : \min \left\{
        \left\{ t - 2 \left( \mathbf{q}^{\top} \mathbf{p} - R^2 \right) \right\} - 4 \left( R^2 - \mathbf{p}^{\top} \mathbf{p} \right) \left( R^2 - \mathbf{q}^{\top} \mathbf{q} \right) : t \in \mathbb{R}_{+} \right\} \leq 0 \right\} \\
        &\stackrel{\textnormal{(a)}}{=}
        \left\{ R \in \left[ 0, \left( \mathbf{q}^{\top} \mathbf{p} \right)^{\frac{1}{2}} \right] : \left( R^2 - \mathbf{p}^{\top} \mathbf{p} \right) \left( R^2 - \mathbf{q}^{\top} \mathbf{q} \right) \geq 0 \right\} \\
        &\quad \cup \left\{ R \in \left( \left( \mathbf{q}^{\top} \mathbf{p} \right)^{\frac{1}{2}}, +\infty \right) : \left( R^2 - \mathbf{q}^{\top} \mathbf{p} \right)^2 - \left( R^2 - \mathbf{p}^{\top} \mathbf{p} \right) \left( R^2 - \mathbf{q}^{\top} \mathbf{q} \right) \leq 0 \right\} \\
        &\stackrel{\textnormal{(b)}}{=} \left[ 0, \left( \mathbf{p}^{\top} \mathbf{p} \right)^{\frac{1}{2}} \right]
        \cup \underbrace{\left\{ R \in \left( \left( \mathbf{q}^{\top} \mathbf{p} \right)^{\frac{1}{2}}, +\infty \right) : R^2 \leq \mathbf{q}^{\top} \mathbf{q}
        - \frac{\left\{ \mathbf{q}^{\top} \left( \mathbf{p} - \mathbf{q} \right) \right\}^2}{\left\| \mathbf{p} - \mathbf{q} \right\|_{2}^2} \right\}}_{= \ \varnothing} \\
        &\stackrel{\textnormal{(c)}}{=} \left[ 0, \left( \mathbf{p}^{\top} \mathbf{p} \right)^{\frac{1}{2}} \right],
    \end{split}
\end{equation}
where the steps (a)--(c) holds due to the following reasons:
\begin{enumerate} [label=(\alph*)]
    \item It's straightforward that
    \begin{equation}
        \label{eqn1.9}
        \begin{split}
            &\min \left\{
            \left\{ t - 2 \left( \mathbf{q}^{\top} \mathbf{p} - R^2 \right) \right\} - 4 \left( R^2 - \mathbf{p}^{\top} \mathbf{p} \right) \left( R^2 - \mathbf{q}^{\top} \mathbf{q} \right) : t \in \mathbb{R}_{+} \right\} \\
            = \ &
            \begin{cases}
                - 4 \left( R^2 - \mathbf{p}^{\top} \mathbf{p} \right) \left( R^2 - \mathbf{q}^{\top} \mathbf{q} \right) & \textnormal{if } \mathbf{q}^{\top} \mathbf{p} - R^2 \geq 0; \\
                4 \left[ \left( R^2 - \mathbf{q}^{\top} \mathbf{p} \right)^2 - \left( R^2 - \mathbf{p}^{\top} \mathbf{p} \right) \left( R^2 - \mathbf{q}^{\top} \mathbf{q} \right) \right] & \textnormal{otherwise.}
            \end{cases}
        \end{split}
    \end{equation}
    \item We have from the relation \eqref{eqn1.6} that
    \begin{equation*}
        \begin{split}
            \left\{ R \in \left[ 0, \left( \mathbf{q}^{\top} \mathbf{p} \right)^{\frac{1}{2}} \right] : \left( R^2 - \mathbf{p}^{\top} \mathbf{p} \right) \left( R^2 - \mathbf{q}^{\top} \mathbf{q} \right) \geq 0 \right\} 
            &= \left\{ R \in \left[ 0, \left( \mathbf{q}^{\top} \mathbf{p} \right)^{\frac{1}{2}} \right] : R^2 \leq \mathbf{p}^{\top} \mathbf{p} \textnormal{ or } R^2 \geq \mathbf{q}^{\top} \mathbf{q} \right\} \\
            &= \left[ 0, \left( \mathbf{p}^{\top} \mathbf{p} \right)^{\frac{1}{2}} \right].
        \end{split}
    \end{equation*}
    \item It suffices to prove that $\mathbf{q}^{\top} \mathbf{p} \geq \mathbf{q}^{\top} \mathbf{q}
    - \frac{\left\{ \mathbf{q}^{\top} \left( \mathbf{p} - \mathbf{q} \right) \right\}^2}{\left\| \mathbf{p} - \mathbf{q} \right\|_{2}^2}$:
    \begin{equation*}
        \begin{split}
            \mathbf{q}^{\top} \mathbf{p} - \left[ \mathbf{q}^{\top} \mathbf{q}
            - \frac{\left\{ \mathbf{q}^{\top} \left( \mathbf{p} - \mathbf{q} \right) \right\}^2}{\left\| \mathbf{p} - \mathbf{q} \right\|_{2}^2} \right]
            =
            \frac{\left( \mathbf{q}^{\top} \mathbf{p} - \mathbf{q}^{\top} \mathbf{q} \right) \left( \mathbf{p}^{\top} \mathbf{p} - \mathbf{p}^{\top} \mathbf{q} \right)}{\left\| \mathbf{p} - \mathbf{q} \right\|_{2}^2}
            \stackrel{\textnormal{(d)}}{\geq} 0,
        \end{split}
    \end{equation*}
    where the step (d) follows from \eqref{eqn1.6}.
\end{enumerate}
\noindent From \eqref{eqn1.8}, we may conclude that
\begin{equation}
    \label{eqn1.10}
    \mathcal{D} \left( \mathbf{p}; \mathbf{q} \right) = \left( \mathbf{p}^{\top} \mathbf{p} \right)^{\frac{1}{2}},
\end{equation}
when $\mathbf{p}^{\top} \mathbf{q} \geq \mathbf{p}^{\top} \mathbf{p}$.
\medskip

\indent \indent \textbf{Case \#3.} $\mathbf{p}^{\top} \mathbf{q} < \min \left\{ \mathbf{p}^{\top} \mathbf{p}, \mathbf{q}^{\top} \mathbf{q} \right\}$: Since $\mathbf{q}^{\top} \left( \mathbf{p} - \mathbf{q} \right) = \mathbf{p}^{\top} \mathbf{q} - \mathbf{q}^{\top} \mathbf{q} < 0$, one can see that $\mathcal{D} \left( \mathbf{p}; \mathbf{q} \right) < \left( \mathbf{q}^{\top} \mathbf{q} \right)^{\frac{1}{2}}$ and $\mathbf{q}^{\top} \mathbf{q} - R^2 > 0$ for every $R \in \Omega = \left[ 0, \mathcal{D} \left( \mathbf{p}; \mathbf{q} \right) \right]$ due to the same reason as in \textbf{Case \#2}. Applying the Schur's complement rule, we arrive at
\begin{equation}
    \label{eqn1.11}
    \begin{split}
        \Omega &=
        \left\{ R \in \mathbb{R}_{+} : \tau + \left\| \mathbf{p} - \mathbf{q} \right\|_{2}^2 - \frac{\left\{ \mathbf{q}^{\top} \left( \mathbf{p} - \mathbf{q} \right) - \frac{\tau}{2} \right\}^2}{\mathbf{q}^{\top} \mathbf{q} - R^2} \geq 0 \textnormal{ for some } \tau \in \mathbb{R}_{+} \right\} \\
        &= \left\{ R \in \mathbb{R}_{+} : \left\{ \tau - 2 \left( \mathbf{q}^{\top} \mathbf{p} - R^2 \right) \right\} - 4 \left( R^2 - \mathbf{p}^{\top} \mathbf{p} \right) \left( R^2 - \mathbf{q}^{\top} \mathbf{q} \right) \leq 0 \textnormal{ for some } \tau \in \mathbb{R}_{+} \right\} \\
        &= \left\{ R \in \mathbb{R}_{+} : \min \left\{
        \left\{ t - 2 \left( \mathbf{q}^{\top} \mathbf{p} - R^2 \right) \right\} - 4 \left( R^2 - \mathbf{p}^{\top} \mathbf{p} \right) \left( R^2 - \mathbf{q}^{\top} \mathbf{q} \right) : t \in \mathbb{R}_{+} \right\} \leq 0 \right\} \\
        &\stackrel{\textnormal{(e)}}{=}
        \left\{ R \in \left[ 0, \left( \mathbf{q}^{\top} \mathbf{p} \right)^{\frac{1}{2}} \right] : \left( R^2 - \mathbf{p}^{\top} \mathbf{p} \right) \left( R^2 - \mathbf{q}^{\top} \mathbf{q} \right) \geq 0 \right\} \\
        &\quad \cup \left\{ R \in \left( \left( \mathbf{q}^{\top} \mathbf{p} \right)^{\frac{1}{2}}, +\infty \right) : \left( R^2 - \mathbf{q}^{\top} \mathbf{p} \right)^2 - \left( R^2 - \mathbf{p}^{\top} \mathbf{p} \right) \left( R^2 - \mathbf{q}^{\top} \mathbf{q} \right) \leq 0 \right\} \\
        &\stackrel{\textnormal{(f)}}{=} \left[ 0, \left( \mathbf{q}^{\top} \mathbf{p} \right)^{\frac{1}{2}} \right]
        \cup \left( \left( \mathbf{q}^{\top} \mathbf{p} \right)^{\frac{1}{2}}, \left[ \mathbf{q}^{\top} \mathbf{q}
        - \frac{\left\{ \mathbf{q}^{\top} \left( \mathbf{p} - \mathbf{q} \right) \right\}^2}{\left\| \mathbf{p} - \mathbf{q} \right\|_{2}^2} \right]^{\frac{1}{2}} \right] \\
        &= \left[ 0, \left[ \mathbf{q}^{\top} \mathbf{q}
        - \frac{\left\{ \mathbf{q}^{\top} \left( \mathbf{p} - \mathbf{q} \right) \right\}^2}{\left\| \mathbf{p} - \mathbf{q} \right\|_{2}^2} \right]^{\frac{1}{2}} \right],
    \end{split}
\end{equation}
where the step (e) makes use of the fact \eqref{eqn1.9}, and the step (f) follows from $\mathbf{p}^{\top} \mathbf{q} < \min \left\{ \mathbf{p}^{\top} \mathbf{p}, \mathbf{q}^{\top} \mathbf{q} \right\}$, which implies $\mathbf{q}^{\top} \mathbf{p} < \mathbf{q}^{\top} \mathbf{q}
- \frac{\left\{ \mathbf{q}^{\top} \left( \mathbf{p} - \mathbf{q} \right) \right\}^2}{\left\| \mathbf{p} - \mathbf{q} \right\|_{2}^2}$. Hence, we obtain
\begin{equation}
    \label{eqn1.12}
    \mathcal{D} \left( \mathbf{p}; \mathbf{q} \right)^2
    = \mathbf{q}^{\top} \mathbf{q}
    - \frac{\left\{ \mathbf{q}^{\top} \left( \mathbf{p} - \mathbf{q} \right) \right\}^2}{\left\| \mathbf{p} - \mathbf{q} \right\|_{2}^2},
\end{equation}
provided that $\mathbf{p}^{\top} \mathbf{q} < \min \left\{ \mathbf{p}^{\top} \mathbf{p}, \mathbf{q}^{\top} \mathbf{q} \right\}$.
\medskip

\indent By taking three pieces \eqref{eqn1.5}, \eqref{eqn1.10}, and \eqref{eqn1.12} collectively, we finally get
\begin{equation*}
    \begin{split}
        \mathcal{D} \left( \mathbf{p}; \mathbf{q} \right)^2 &=
        \begin{cases}
            \mathbf{q}^{\top} \mathbf{q} & \textnormal{if } \mathbf{p}^{\top} \mathbf{q} \geq \mathbf{q}^{\top} \mathbf{q}; \\
            \mathbf{p}^{\top} \mathbf{p} & \textnormal{if } \mathbf{p}^{\top} \mathbf{q} \geq \mathbf{p}^{\top} \mathbf{p}; \\
            \mathbf{q}^{\top} \mathbf{q}
            - \frac{\left\{ \mathbf{q}^{\top} \left( \mathbf{p} - \mathbf{q} \right) \right\}^2}{\left\| \mathbf{p} - \mathbf{q} \right\|_{2}^2} & \textnormal{if } \mathbf{p}^{\top} \mathbf{q} < \min \left\{ \mathbf{p}^{\top} \mathbf{p}, \mathbf{q}^{\top} \mathbf{q} \right\},
        \end{cases} \\
        &=
        \begin{cases}
            \mathbf{q}^{\top} \mathbf{q} & \textnormal{if } \mathbf{p}^{\top} \mathbf{q} > \mathbf{q}^{\top} \mathbf{q}; \\
            \mathbf{p}^{\top} \mathbf{p} & \textnormal{if } \mathbf{p}^{\top} \mathbf{q} > \mathbf{p}^{\top} \mathbf{p}; \\
            \mathbf{q}^{\top} \mathbf{q}
            - \frac{\left\{ \mathbf{q}^{\top} \left( \mathbf{p} - \mathbf{q} \right) \right\}^2}{\left\| \mathbf{p} - \mathbf{q} \right\|_{2}^2} & \textnormal{if } \mathbf{p}^{\top} \mathbf{q} \leq \min \left\{ \mathbf{p}^{\top} \mathbf{p}, \mathbf{q}^{\top} \mathbf{q} \right\},
        \end{cases}
    \end{split}
\end{equation*}
and this result coincides with the result in  \emph{Exercise 9.3} in \cite{calafiore2014optimization} as desired.
}
\end{problem}

\begin{problem} [\emph{Exercise 11.7} in \cite{calafiore2014optimization}]
\label{problem2}
\normalfont{\ \\
\indent (1) We first prove the ``only if'' direction. Assume that $\mathbf{X} \in \mathcal{S}^n$ satisfies $f_k (\mathbf{X}) = \sum_{i=1}^{k} \lambda_i (\mathbf{X}) \leq t$. Here, $\mathcal{S}^n$ denotes the $\mathbb{R}$-vector space of all $n \times n$ real symmetric matrices. Let $\mathbf{X} = \mathbf{U \Sigma U}^{\top}$ be the spectral decomposition of $\mathbf{X} \in \mathcal{S}^n$, where $\mathbf{U} := \begin{bmatrix} \mathbf{u}_1 & \mathbf{u}_2 & \cdots & \mathbf{u}_n \end{bmatrix} \in \mathcal{O}(n)$ and $\mathbf{\Sigma} := \textsf{diag} \left( \lambda_1 (\mathbf{X}), \lambda_2 (\mathbf{X}), \cdots, \lambda_n (\mathbf{X}) \right) \in \mathbb{R}^{n \times n}$ together with
\begin{equation*}
    \lambda_1 (\mathbf{X}) \geq \lambda_2 (\mathbf{X}) \geq \cdots \geq \lambda_n (\mathbf{X}).
\end{equation*}
Now, let $s := \lambda_k (\mathbf{X}) \in \mathbb{R}$ and
\begin{equation*}
    \mathbf{Z} = \mathbf{U} \textsf{diag} \left( d_1, d_2, \cdots, d_n \right) \mathbf{U}^{\top} 
    = \sum_{i=1}^{n} d_i \mathbf{u}_i \mathbf{u}_{i}^{\top} \in \mathcal{S}^n,
\end{equation*}
where
\begin{equation*}
    d_i :=
    \begin{cases}
        \lambda_i (\mathbf{X}) - \lambda_k (\mathbf{X}) & \textnormal{if } 1 \leq i \leq k; \\
        0 & \textnormal{otherwise.}
    \end{cases}
\end{equation*}
Since $d_i$ are all non-negative, we have
\begin{equation*}
    \mathbf{v}^{\top} \mathbf{Z} \mathbf{v}
    = \sum_{i=1}^{n} d_i \left( \mathbf{u}_{i}^{\top} \mathbf{v} \right)^2 \geq 0,\ \forall \mathbf{v} \in \mathbb{R}^n,
\end{equation*}
which implies that $\mathbf{Z} \succeq \mathbf{O}_{n \times n}$. Next, we claim that $t - ks - \textsf{Trace}(\mathbf{Z}) \geq 0$. This can be justified as follows:
\begin{equation*}
    \begin{split}
        t - ks - \textsf{Trace}(\mathbf{Z})
        &= t - k \lambda_k (\mathbf{X}) - \sum_{i=1}^{k} \left\{ \lambda_i (\mathbf{X}) - \lambda_k (\mathbf{X}) \right\} \\
        &= t - \sum_{i=1}^{k} \lambda_i (\mathbf{X}) \\
        &= t - f_k (\mathbf{X}) \geq 0.
    \end{split}
\end{equation*}
\noindent Finally, it remains to show that $\mathbf{Z} - \mathbf{X} + s \mathbf{I}_n \succeq \mathbf{O}_{n \times n}$. We first observe that
\begin{equation}
    \label{eqn2.1}
    \begin{split}
        \mathbf{Z} - \mathbf{X} + s \mathbf{I}_n
        &= \mathbf{U} \textsf{diag} \left( d_1 - \lambda_1 (\mathbf{X}) + s, d_2 - \lambda_2 (\mathbf{X}) + s, \cdots, d_n - \lambda_n (\mathbf{X}) + s \right) \mathbf{U}^{\top} \\
        &= \sum_{i=1}^{n} \left\{ d_i - \lambda_i (\mathbf{X}) + s \right\} \mathbf{u}_i \mathbf{u}_{i}^{\top}.
    \end{split}
\end{equation}
Since
\begin{equation*}
    d_i - \lambda_i (\mathbf{X}) + s =
    \begin{cases}
        0 & \textnormal{if } 1 \leq i \leq k; \\
        \lambda_k (\mathbf{X}) - \lambda_i (\mathbf{X}) & \textnormal{otherwise,}
    \end{cases}
\end{equation*}
$d_i - \lambda_i + s \geq 0$ for every $i \in [n]$. So for every $\mathbf{v} \in \mathbb{R}^n$, we have from \eqref{eqn2.1} that
\begin{equation*}
    \mathbf{v}^{\top} \left( \mathbf{Z} - \mathbf{X} + s \mathbf{I}_n \right) \mathbf{v}
    = \sum_{i=1}^{n} \left( d_i - \lambda_i + s \right) \left( \mathbf{u}_{i}^{\top} \mathbf{v} \right)^2 \geq 0,\ \forall \mathbf{v} \in \mathbb{R}^n,
\end{equation*}
thereby $\mathbf{Z} - \mathbf{X} + s \mathbf{I}_n \succeq \mathbf{O}_{n \times n}$ as desired.
\medskip

\indent It's time to prove the ``if'' part. We know that
\begin{equation*}
    \mathbf{Z} = \left( \mathbf{Z} - \mathbf{X} + s \mathbf{I}_n \right) + \left( \mathbf{X} - s \mathbf{I}_n \right)
\end{equation*}
and $\mathbf{Z} - \mathbf{X} + s \mathbf{I}_n \succeq \mathbf{O}_{n \times n}$. By applying inequality (4.6) in \cite{calafiore2014optimization} (which is an immediate consequence of \emph{Corollary 4.2} in \cite{calafiore2014optimization}), we obtain
\begin{equation}
    \label{eqn2.2}
    \lambda_i (\mathbf{Z}) \geq \lambda_i \left( \mathbf{X} - s \mathbf{I}_n \right)
    = \lambda_i (\mathbf{X}) - s,\ \forall i \in [n].
\end{equation}
Thus, we arrive at
\begin{equation*}
    \begin{split}
        f_k (\mathbf{X})
        &= \sum_{i=1}^{k} \lambda_i (\mathbf{X}) \\
        &\stackrel{\textnormal{(a)}}{\leq} \sum_{i=1}^{k} \lambda_i (\mathbf{Z}) + ks \\
        &\stackrel{\textnormal{(b)}}{\leq} \sum_{i=1}^{n} \lambda_i (\mathbf{Z}) + ks \\
        &= \textsf{Trace}(\mathbf{Z}) + ks \\
        &\leq t,
    \end{split}
\end{equation*}
where the step (a) follows from the inequality \eqref{eqn2.2}, and the step (b) holds since $\mathbf{Z} \geq \mathbf{O}_{n \times n}$, which implies that $\lambda_{\min} (\mathbf{Z}) \geq 0$. This completes the proof of part (1) of Problem \ref{problem2}.
\medskip

\indent (2) We claim that $f_k (\cdot) : \mathcal{S}^n \rightarrow \mathbb{R}$ is a convex function. To begin with, it's evident that $\textsf{dom} \left( f_k \right) = \mathcal{S}^n$ is convex in $\mathcal{S}^n$. We claim that the epigraph of the function $f_k (\cdot) : \mathcal{S}^n \rightarrow \mathbb{R}$,
\begin{equation*}
    \textsf{epi} \left( f_k \right) :=
    \left\{ \left( \mathbf{X}, t \right) \in \mathcal{S}^n \times \mathbb{R} : f_k (\mathbf{X}) \leq t \right\},
\end{equation*}
is a convex subset of the $\mathbb{R}$-vector space $\mathcal{S}^n \times \mathbb{R}$. To this end, we choose any $\left( \mathbf{X}_0, t_0 \right), \left( \mathbf{X}_1, t_1 \right) \in \textsf{epi} \left( f_k \right)$. Due to the ``only if'' part of (1), there exist $\mathbf{Z}_0, \mathbf{Z}_1 \in \mathcal{S}^n$ and $s_0, s_1 \in \mathbb{R}$ such that
\begin{equation}
    \label{eqn2.3}
    t_i - k s_i - \textsf{Trace} \left( \mathbf{Z}_i \right) \geq 0; \quad
    \mathbf{Z}_i \succeq \mathbf{O}_{n \times n}; \quad
    \mathbf{Z}_i - \mathbf{X}_i + s_i \mathbf{I}_n \succeq \mathbf{O}_{n \times n},\ \forall i \in \left\{ 0, 1 \right\}.
\end{equation}
So it follows from the fact \eqref{eqn2.3} that for every $\theta \in [0, 1]$,
\begin{equation*}
    \begin{split}
        \left\{ \left( 1 - \theta \right) t_0 + \theta t_1 \right\} - k \left\{ \left( 1 - \theta \right) s_0 + \theta s_1 \right\} - 
        \textsf{Trace} \left\{ \left( 1 - \theta \right) \mathbf{Z}_0 + \theta \mathbf{Z}_1 \right\} &\geq 0; \\
        \left( 1 - \theta \right) \mathbf{Z}_0 + \theta \mathbf{Z}_1 &\succeq \mathbf{O}_{n \times n}; \\
        \left\{ \left( 1 - \theta \right) \mathbf{Z}_0 + \theta \mathbf{Z}_1 \right\} - \left\{ \left( 1 - \theta \right) \mathbf{X}_0 + \theta \mathbf{X}_1 \right\} + \left\{ \left( 1 - \theta \right) s_0 + \theta s_1 \right\} \mathbf{I}_n & \succeq \mathbf{O}_{n \times n}.
    \end{split}
\end{equation*}
Hence, we have $f_k \left\{ \left( 1 - \theta \right) \mathbf{X}_0 + \theta \mathbf{X}_1 \right\} \leq \left( 1 - \theta \right) t_0 + \theta t_1$, which implies
\begin{equation*}
    \left( \left( 1 - \theta \right) \mathbf{X}_0 + \theta \mathbf{X}_1, \left( 1 - \theta \right) t_0 + \theta t_1 \right)
    = \left( 1 - \theta \right) \left( \mathbf{X}_0, t_0 \right) + \theta \left( \mathbf{X}_1, t_1 \right) \in \textsf{epi} \left( f_k \right)
\end{equation*}
for every $\theta \in [0, 1]$, by the ``if'' part of (1). Hence, the epigraph of the function $f_k (\cdot) : \mathcal{S}^n \rightarrow \mathbb{R}$ is a convex subset of the $\mathbb{R}$-vector space $\mathcal{S}^n \times \mathbb{R}$. As a result, this leads us to the convexity of the function $f_k (\cdot) : \mathcal{S}^n \rightarrow \mathbb{R}$. 
\medskip

\indent However, the function $f_k (\cdot) : \mathcal{S}^n \rightarrow \mathbb{R}$ is NOT a norm on $\mathcal{S}^n$ since it can attain negative real-values. For instance, it's straightforward to see that $- \mathbf{I}_n \in \mathcal{S}^n$ and
\begin{equation*}
    f_k \left( - \mathbf{I}_n \right)
    = \sum_{i=1}^{k} \lambda_i \left( - \mathbf{I}_n \right) 
    = - k < 0.
\end{equation*}

\indent (3) For every $k \in \left[ \min \left\{ m, n \right\} \right]$, we define a function $\varphi_k (\cdot) : \mathbb{R}^{m \times n} \rightarrow \mathbb{R}_{+}$ by
\begin{equation*}
    \varphi_k (\mathbf{X}) := \sum_{i=1}^{k} \sigma_i (\mathbf{X}),
\end{equation*}
where $\sigma_i (\mathbf{X})$ denotes the $i$-th largest singular value of $\mathbf{X} \in \mathbb{R}^{m \times n}$ for $i \in \left[ \min \left\{ m, n \right\} \right]$. Also, we define a map $\Phi(\cdot) : \mathbb{R}^{m \times n} \rightarrow \mathcal{S}^{m + n}$ by
\begin{equation*}
    \Phi (\mathbf{X}) :=
    \begin{bmatrix}
        \mathbf{O}_{m \times m} & \mathbf{X} \\
        \mathbf{X}^{\top} & \mathbf{O}_{n \times n}
    \end{bmatrix}.
\end{equation*}
It's clear that $\Phi(\cdot) : \mathbb{R}^{m \times n} \rightarrow \mathcal{S}^{m + n}$ is an $\mathbb{R}$-linear map. At this point, we would like to prove the following crucial result:

\begin{lemma}
\label{lemma1}
For every $\mathbf{X} \in \mathbb{R}^{m \times n}$, the eigenvalues of $\Phi (\mathbf{X}) \in \mathcal{S}^{m + n}$ are given by
\begin{equation}
    \label{eqn2.4}
    \sigma_1 (\mathbf{X}) \geq \sigma_2 (\mathbf{X}) \geq \cdots \geq \sigma_r (\mathbf{X}) > 
    \underbrace{0 = 0 = \cdots = 0}_{(m+n - 2r) \textnormal{ zeroes}} > - \sigma_r (\mathbf{X}) \geq \cdots \geq - \sigma_2 (\mathbf{X})
    \geq - \sigma_1 (\mathbf{X}),
\end{equation}
where $r := \textnormal{\textsf{rank}}(\mathbf{X}) \leq \min \left\{ m, n \right\}$.
\end{lemma}

\begin{proof} [Proof of Lemma \ref{lemma1}]
To begin with, let $\mathbf{X} = \mathbf{U} \tilde{\mathbf{\Sigma}} \mathbf{V}^{\top}$ be a singular value decomposition of $\mathbf{X} \in \mathbb{R}^{m \times n}$, where $\mathbf{U} \in \mathcal{O}(m)$, $\mathbf{V} \in \mathcal{O}(n)$, and $\tilde{\mathbf{\Sigma}} := \begin{bmatrix} \mathbf{\Sigma} & \mathbf{O}_{r \times (n-r)} \\
\mathbf{O}_{(m-r) \times r} & \mathbf{O}_{(m-r) \times (n-r)} \end{bmatrix} \in \mathbb{R}^{m \times n}$, $\mathbf{\Sigma} := \textsf{diag} \left( \sigma_1 (\mathbf{X}), \sigma_2 (\mathbf{X}), \cdots, \sigma_r (\mathbf{X}) \right) \in \mathbb{R}^{r \times r}$. Then it holds that
\begin{equation}
    \label{eqn2.5}
    \begin{split}
        \begin{bmatrix} \mathbf{U} & \mathbf{O}_{m \times n} \\ \mathbf{O}_{n \times m} & \mathbf{V} \end{bmatrix}^{\top} \Phi (\mathbf{X}) \begin{bmatrix} \mathbf{U} & \mathbf{O}_{m \times n} \\ \mathbf{O}_{n \times m} & \mathbf{V} \end{bmatrix} 
        &= \begin{bmatrix} \mathbf{U}^{\top} & \mathbf{O}_{m \times n} \\ \mathbf{O}_{n \times m} & \mathbf{V}^{\top} \end{bmatrix}
        \begin{bmatrix}
        \mathbf{O}_{m \times m} & \mathbf{U} \tilde{\mathbf{\Sigma}} \mathbf{V}^{\top} \\ \mathbf{V} \tilde{\mathbf{\Sigma}}^{\top} \mathbf{U}^{\top} & \mathbf{O}_{n \times n}
        \end{bmatrix}
        \begin{bmatrix} \mathbf{U} & \mathbf{O}_{m \times n} \\ \mathbf{O}_{n \times m} & \mathbf{V} \end{bmatrix} \\
        &= \begin{bmatrix}
        \mathbf{O}_{m \times m} & \tilde{\mathbf{\Sigma}} \\ \tilde{\mathbf{\Sigma}}^{\top} & \mathbf{O}_{n \times n}
        \end{bmatrix} \\
        &= \sum_{i=1}^{r} \sigma_i (\mathbf{X}) \left\{ \mathbf{e}_{i}^{(m+n)} \left( \mathbf{e}_{m+i}^{(m+n)} \right)^{\top} + \mathbf{e}_{m+i}^{(m+n)} \left( \mathbf{e}_{i}^{(m+n)} \right)^{\top} \right\},
    \end{split}
\end{equation}
where $\mathbf{e}_{j}^{(m+n)} \in \mathbb{R}^{m+n}$ denotes the $j$-th unit vector in $\mathbb{R}^{m+n}$ for each $j \in \left[ m+n \right]$. Set $\mathbf{Q} := \begin{bmatrix} \mathbf{U} & \mathbf{O}_{m \times n} \\ \mathbf{O}_{n \times m} & \mathbf{V} \end{bmatrix} \in \mathcal{O}(m+n)$. Then from the equation \eqref{eqn2.5}, one can obtain the following observations:
\begin{equation}
    \label{eqn2.6}
    \begin{split}
        \left( \mathbf{Q}^{\top} \Phi (\mathbf{X}) \mathbf{Q} \right) \mathbf{e}_{i}^{(m+n)} =
        \begin{cases}
            \sigma_i (\mathbf{X}) \cdot \mathbf{e}_{m+i}^{(m+n)} & \textnormal{if } 1 \leq i \leq r; \\
            \mathbf{0} \in \mathbb{R}^{m+n} & \textnormal{if } r+1 \leq i \leq m,
        \end{cases}
    \end{split}
\end{equation}
and
\begin{equation}
    \label{eqn2.7}
    \begin{split}
        \left( \mathbf{Q}^{\top} \Phi (\mathbf{X}) \mathbf{Q} \right) \mathbf{e}_{m+j}^{(m+n)} =
        \begin{cases}
            \sigma_i (\mathbf{X}) \cdot \mathbf{e}_{j}^{(m+n)} & \textnormal{if } 1 \leq j \leq r; \\
            \mathbf{0} \in \mathbb{R}^{m+n} & \textnormal{if } r+1 \leq j \leq n.
        \end{cases}
    \end{split}
\end{equation}
From the observations \eqref{eqn2.6} and \eqref{eqn2.7}, one can see that
\begin{equation}
    \label{eqn2.8}
    \left( \mathbf{Q}^{\top} \Phi (\mathbf{X}) \mathbf{Q} \right) \cdot \frac{1}{\sqrt{2}}
    \left( \mathbf{e}_{i}^{(m+n)} + \mathbf{e}_{m+i}^{(m+n)} \right) = 
    \sigma_i (\mathbf{X}) \cdot \frac{1}{\sqrt{2}}
    \left( \mathbf{e}_{i}^{(m+n)} + \mathbf{e}_{m+i}^{(m+n)} \right),\ \forall i \in [r],
\end{equation}
and 
\begin{equation}
    \label{eqn2.9}
    \left( \mathbf{Q}^{\top} \Phi (\mathbf{X}) \mathbf{Q} \right) \cdot \frac{1}{\sqrt{2}}
    \left( \mathbf{e}_{j}^{(m+n)} - \mathbf{e}_{m+j}^{(m+n)} \right) = 
    - \sigma_j (\mathbf{X}) \cdot \frac{1}{\sqrt{2}}
    \left( \mathbf{e}_{j}^{(m+n)} - \mathbf{e}_{m+j}^{(m+n)} \right),\ \forall j \in [r].
\end{equation}
By taking four pieces \eqref{eqn2.6}--\eqref{eqn2.9} collectively, we may conclude that
\begin{equation*}
    \begin{split}
        \mathcal{B} := \ &
        \left\{ \frac{1}{\sqrt{2}}
        \left( \mathbf{e}_{i}^{(m+n)} + \mathbf{e}_{m+i}^{(m+n)} \right) : i \in [r] \right\}
        \cup \left\{ \mathbf{e}_{i}^{(m+n)} : i \in \left[ r+1; m \right] \right\} \\
        &\cup \left\{ \mathbf{e}_{m+j}^{(m+n)} : j \in \left[ r+1; n \right] \right\}
        \cup \left\{ \frac{1}{\sqrt{2}}
        \left( \mathbf{e}_{j}^{(m+n)} - \mathbf{e}_{m+j}^{(m+n)} \right) : j \in [r] \right\}
    \end{split}
\end{equation*}
forms an orthonormal basis for $\mathbb{R}^{m+n}$ consisting of eigenvectors of the matrix $\mathbf{Q}^{\top} \Phi (\mathbf{X}) \mathbf{Q} \in \mathcal{S}^{m+n}$, where
\begin{equation*}
    \left[ a; b \right] := \left\{ a, a+1, \cdots, b-1, b \right\},\ \forall a \leq b \textnormal{ in } \mathbb{Z},
\end{equation*}
with the corresponding eigenvalues given in \eqref{eqn2.4}. So, the eigenvalues of $\Phi (\mathbf{X}) \in \mathcal{S}^{m+n}$ are given as \eqref{eqn2.4} as we claimed.

\end{proof}

\indent By Lemma \ref{lemma1}, we have for every $k \in \left[ \min \left\{ m, n \right\} \right]$,
\begin{equation}
    \label{eqn2.10}
    \varphi_k (\mathbf{X}) = \sum_{i=1}^{k} \sigma_i (\mathbf{X})
    = \sum_{i=1}^{k} \lambda_i \left( \Phi (\mathbf{X}) \right) 
    = f_k \left( \Phi(\mathbf{X}) \right),\ \forall \mathbf{X} \in \mathbb{R}^{m \times n}.
\end{equation}
From the fact \eqref{eqn2.10}, one can easily derive the following conclusions, which are also generalized versions of the results in part (1) and (2):
\begin{enumerate} [label=(\roman*)]
    \item For every $k \in \left[ \min \left\{ m, n \right\} \right]$, one has
    \begin{equation*}
        \begin{split}
            \left\{ \mathbf{X} \in \mathbb{R}^{m \times n} : \varphi_k (\mathbf{X}) \leq t \right\}
            \stackrel{\textnormal{(c)}}{=} \ & \left\{ \mathbf{X} \in \mathbb{R}^{m \times n} : f_k \left( \Phi (\mathbf{X}) \right) \leq t \right\} \\
            \stackrel{\textnormal{(d)}}{=} \ & \left\{ \mathbf{X} \in \mathbb{R}^{m \times n} : \exists \tilde{\mathbf{Z}} \in \mathcal{S}^{m+n} \ \& \ s \in \mathbb{R} \textnormal{ such that } t - ks - \textsf{Trace} \left( \tilde{\mathbf{Z}} \right) \geq 0; \right. \\
            &\left. \tilde{\mathbf{Z}} \succeq \mathbf{O}_{(m+n) \times (m+n)}; \ \tilde{\mathbf{Z}} - \Phi (\mathbf{X}) + s \mathbf{I}_{m+n}
            \succeq \mathbf{O}_{(m+n) \times (m+n)} \right\},
        \end{split}
    \end{equation*}
    where the step (c) holds due to the equation \eqref{eqn2.10}, and the step (d) follows by applying the part (1) for $\Phi (\mathbf{X}) \in \mathcal{S}^{m+n}$;
    \item The function $\varphi_k (\cdot): \mathbb{R}^{m \times n} \rightarrow \mathbb{R}_{+}$ is a convex function. To this end, we first take a closer inspection on the epigraph $\textsf{epi} \left( \varphi_k \right)$ of the function $\varphi_k (\cdot): \mathbb{R}^{m \times n} \rightarrow \mathbb{R}_{+}$:
    \begin{equation}
        \label{eqn2.11}
        \begin{split}
            \textsf{epi} \left( \varphi_k \right)
            &= \left\{ \left( \mathbf{X}, t \right) \in \mathbb{R}^{m \times n} \times \mathbb{R}: \varphi_k (\mathbf{X}) \leq t \right\} \\
            &= \left\{ \left( \mathbf{X}, t \right) \in \mathbb{R}^{m \times n} \times \mathbb{R}: f_k \left( \Phi (\mathbf{X}) \right) \leq t \right\} \\
            &= \left\{ \left( \mathbf{X}, t \right) \in \mathbb{R}^{m \times n} \times \mathbb{R}: \left( \Phi (\mathbf{X}), t \right) \in \textsf{epi} \left( f_k \right) \right\}. \\
        \end{split}
    \end{equation}
    We know from the part (2) that the epigraph $\textsf{epi} \left( f_k \right)$ of the function $f_k (\cdot) : \mathcal{S}^{m+n} \rightarrow \mathbb{R}$ is a convex subset of $\mathcal{S}^{m+n} \times \mathbb{R}$ due to the convexity of $f_k (\cdot) : \mathcal{S}^{m+n} \rightarrow \mathbb{R}$. As $\Phi (\cdot) : \mathbb{R}^{m \times n} \rightarrow \mathcal{S}^{m+n}$ is an $\mathbb{R}$-linear map, it's clear that the map
    \begin{equation}
        \label{eqn2.12}
        \left( \mathbf{X}, t \right) \in \mathbb{R}^{m \times n} \times \mathbb{R} \mapsto \left( \Phi (\mathbf{X}), t \right) \in \mathcal{S}^{m+n} \times \mathbb{R}
    \end{equation}
    is also an $\mathbb{R}$-linear map. So from \eqref{eqn2.11}, it follows that $\textsf{epi} \left( \varphi_k \right)$ is a convex subset of $\mathbb{R}^{m \times n} \times \mathbb{R}$ since it is an inverse image of the convex subset $\textsf{epi} \left( f_k \right)$ of $\mathcal{S}^{m+n} \times \mathbb{R}$ under the $\mathbb{R}$-linear map \eqref{eqn2.12}. Hence, the function $\varphi_k (\cdot): \mathbb{R}^{m \times n} \rightarrow \mathbb{R}_{+}$ is a convex function;
    \item Finally, unlike the case of the function $f_k (\cdot)$, the function $\varphi_k (\cdot): \mathbb{R}^{m \times n} \rightarrow \mathbb{R}_{+}$ is not only just a convex function, but also a norm on $\mathbb{R}^{m \times n}$:
    \begin{itemize}
        \item \textbf{Positive definiteness of the map $\varphi_k (\cdot): \mathbb{R}^{m \times n} \rightarrow \mathbb{R}_{+}$:} It's evident that if $\mathbf{X} = \mathbf{O}_{m \times n}$, then $\varphi_k (\mathbf{X}) = 0$ for every $k \in \left[ \min \left\{ m, n \right\} \right]$. Conversely, assume that $\varphi_k (\mathbf{X}) = 0$ for $\mathbf{X} \in \mathbb{R}^{m \times n}$. Then we have $\sigma_{1} (\mathbf{X}) = 0$, which clearly implies that $\sigma_i (\mathbf{X}) = 0$ for all $i \in \left[ \min \left\{ m, n \right\} \right]$. Thus, we have $\mathbf{X} = \mathbf{O}_{m \times n}$ by considering its singular value decomposition. This establishes the positive definiteness of the map $\varphi_k (\cdot): \mathbb{R}^{m \times n} \rightarrow \mathbb{R}_{+}$;
        \item \textbf{Absolute homogeneity of the map $\varphi_k (\cdot): \mathbb{R}^{m \times n} \rightarrow \mathbb{R}_{+}$:} Let $\mathbf{X} = \mathbf{U} \tilde{\mathbf{\Sigma}} \mathbf{V}^{\top}$ be a singular value decomposition of $\mathbf{X} \in \mathbb{R}^{m \times n}$, where $\mathbf{U} \in \mathcal{O}(m)$, $\mathbf{V} \in \mathcal{O}(n)$, and $\tilde{\mathbf{\Sigma}} := \begin{bmatrix} \mathbf{\Sigma} & \mathbf{O}_{r \times (n-r)} \\
        \mathbf{O}_{(m-r) \times r} & \mathbf{O}_{(m-r) \times (n-r)} \end{bmatrix} \in \mathbb{R}^{m \times n}$, $\mathbf{\Sigma} := \textsf{diag} \left( \sigma_1 (\mathbf{X}), \sigma_2 (\mathbf{X}), \cdots, \sigma_r (\mathbf{X}) \right) \in \mathbb{R}^{r \times r}$. Then for any $\alpha \in \mathbb{R}$, $\alpha \mathbf{X} = \left( \textsf{sign} (\alpha) \mathbf{U} \right) \left( \left| \alpha \right| \tilde{\mathbf{\Sigma}} \right) \mathbf{V}^{\top}$ is a singular value decomposition of $\alpha \mathbf{X} \in \mathbb{R}^{m \times n}$, where
        \begin{equation*}
            \textsf{sign}(\alpha) :=
            \begin{cases}
                +1 & \textnormal{if } \alpha > 0; \\
                0 & \textnormal{if } \alpha = 0; \\
                -1 & \textnormal{otherwise.}
            \end{cases}
        \end{equation*}
        Thus, we obtain $\sigma_i \left( \alpha \mathbf{X} \right) = \left| \alpha \right| \cdot \sigma_i (\mathbf{X})$ for all $i \in \left[ \min \left\{ m, n \right\} \right]$. Hence, it holds that for every $k \in \left[ \min \left\{ m, n \right\} \right]$,
        \begin{equation}
            \label{eqn2.13}
            \varphi_k \left( \alpha \mathbf{X} \right)
            = \sum_{i=1}^{k} \sigma_i \left( \alpha \mathbf{X} \right)
            = \sum_{i=1}^{k} \left| \alpha \right| \cdot \sigma_i (\mathbf{X})
            = \left| \alpha \right| \varphi_k (\mathbf{X})
        \end{equation}
        for every $\alpha \in \mathbb{R}$ and $\mathbf{X} \in \mathbb{R}^{m \times n}$. This establishes the absolute homogeneity of $\varphi_k (\cdot): \mathbb{R}^{m \times n} \rightarrow \mathbb{R}_{+}$;
        \item \textbf{Sub-additivity of the map $\varphi_k (\cdot): \mathbb{R}^{m \times n} \rightarrow \mathbb{R}_{+}$:} Choose any matrices $\mathbf{X}, \mathbf{Y} \in \mathbb{R}^{m \times n}$. Then we obtain
        \begin{equation*}
            \begin{split}
                \varphi_k \left( \mathbf{X} + \mathbf{Y} \right)
                \stackrel{\textnormal{(e)}}{=} 2 \varphi_k \left( \frac{\mathbf{X} + \mathbf{Y}}{2} \right)
                \stackrel{\textnormal{(f)}}{\leq} 2 \cdot \frac{\varphi_k (\mathbf{X}) + \varphi_k (\mathbf{Y})}{2} = \varphi_k (\mathbf{X}) + \varphi_k (\mathbf{Y}),
            \end{split}
        \end{equation*}
        where the step (e) follows from the absolute homogeneity \eqref{eqn2.13} of the map $\varphi_k (\cdot): \mathbb{R}^{m \times n} \rightarrow \mathbb{R}_{+}$, and the step (f) makes use of the convexity of the function $\varphi_k (\cdot): \mathbb{R}^{m \times n} \rightarrow \mathbb{R}_{+}$ (which was argued in (\romannumeral 2)). Hence, the map $\varphi_k (\cdot): \mathbb{R}^{m \times n} \rightarrow \mathbb{R}_{+}$ is sub-additive.
    \end{itemize}
    Thanks to the above observations, one can conclude that the function $\varphi_k (\cdot): \mathbb{R}^{m \times n} \rightarrow \mathbb{R}_{+}$ is a norm on $\mathbb{R}^{m \times n}$, while we have shown that the function $f_k (\cdot) : \mathcal{S}^n \rightarrow \mathbb{R}$ cannot be a norm on $\mathcal{S}^n$.
\end{enumerate}
}
\end{problem}

\newpage

\bibliographystyle{plain}
\bibliography{main.bib}

\end{document}